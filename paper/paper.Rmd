---
title: "The Hot Hand Fallacy in Call of Duty Search and Destroy"
authors:
  - name: Person One
    email: personone@domain.com
  - name: Person Two
    email: persontwo@domain.com
abstract: |
  Our research investigates patterns in round win percentages in professional Search and Destroy (SnD) matches of the popular first-person shooter game Call of Duty (CoD). First, we find evidence supporting the hypothesis that round win probability can be modeled as a constant across rounds in the series, although not at the naive 50%. Second, we examine the proportion of round wins immediately following a win streak. Given streak length, series length, and series winner, we find no evidence that post-streak win proportion is significantly different than expected, suggesting that the perception of momentum at the team level is deceiving. Wald-Wolfowitz run tests also fail to provide evidence for the hot hand phenomenon.
bibliography: references.bib
keywords:
  - esports
  - Call of Duty
  - hot hand
  - runs test
output:
  rticles::arxiv_article:
    includes:
      in_header: preamble.tex
---

```{r}
#| label: setup
#| include: false
#| echo: false
#| eval: true
knitr::opts_chunk$set(
  eval = FALSE,
  echo = FALSE,
  include = FALSE,
  message = FALSE,
  warning = FALSE
)
```

```{r}
#| label: setup-code
library(readr)
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)

## i annotate when i use these with ::
library(broom)
library(randtests)
library(withr)

cod_rounds <- read_csv('data/cod_rounds.csv')
```

```{r}
#| label: descriptive-cod-snd-stats
cod_n_series <- cod_rounds |> 
  distinct(series_id) |> 
  nrow()

cod_n_rounds <- cod_rounds |> 
  distinct(series_id, round) |> 
  nrow()

cod_o_win_prop <- cod_rounds |> 
  filter(is_offense) |> 
  count(win_round) |> 
  mutate(prop = n / sum(n)) |> 
  filter(win_round) |> 
  pull(prop)
cod_o_win_prop^2 * (1 - cod_o_win_prop)
cod_o_win_prop * (1 - cod_o_win_prop)^2

cod_o_win_prop_by_game <- cod_rounds |> 
  filter(is_offense) |> 
  count(game, win_round) |> 
  group_by(game) |> 
  mutate(prop = n / sum(n)) |> 
  ungroup() |> 
  filter(win_round) |> 
  select(game, n, prop)

cod_n_series_lan <- cod_rounds |> 
  filter(event |> str_detect('C[Hh][Aa][Mm][Pp]|Major')) |> 
  distinct(series_id) |> 
  nrow()

cod_win_series_prop <- cod_rounds |> 
  distinct(series_id, win_series, n_rounds) |> 
  count(n_rounds) |> 
  mutate(prop = n / sum(n)) |> 
  arrange(n_rounds)

cod_rounds |> 
  filter(round == 1) |> 
  count(game, team, is_offense) |> 
  group_by(game, team) |> 
  mutate(prop = n / sum(n)) |> 
  ungroup() |>
  filter(is_offense) %>% 
  arrange(-prop)
```

# Introduction

## Call of Duty Search and Destroy

Call of Duty (CoD), first released in 2003, is one of the most popular first-person shooter (FPS) video game franchises of all-time. The most popular mode in the competitive scene is "Search and Destroy" (SnD), in which one team tries to destroy one of two designated bomb sites on the map.[^1]

[^1]: SnD bears resemblance to "Bomb Defusal" in Counter-Strike and "Plant/Defuse" in Valorant, two other FPS games played in more popular professional leagues.

In professional CoD SnD, the two teams[^2] take turns playing offense and defense every round. The first to six round wins (best-of-11 round format) is declared the series winner.[^3] A round can end in one of four ways:

[^2]: In the 2020 season, teams played with five players each; in the 2021 and 2022 seasons, teams played with four players each.

[^3]: A short list of rules that govern timing follows.

    -   Each round has a 90 second time limit, not counting the potential extension granted if a bomb is planted
    -   A bomb plant takes five seconds. The timer resets if the player stops planting site prior to completing it.
    -   A bomb defuse takes seven seconds. The timer resets if the player "drops" the bomb.
    -   Once the bomb is planted, the round timer is set to 45 seconds.

1.  One team eliminates all members of the other team prior to a bomb plant. (Eliminating team wins.)
2.  The offensive team eliminates all members of the defensive team after a bomb plant. (Offense wins.)
3.  The defense defuses the bomb after a bomb plant. (Defense wins.)
4.  The offense does not make a plant by the time the round timer ends. (Defense wins.)

We adopt the terminology "series" to refer to a single best-of-11 matchup, so as to mirror the terminology of playoff series in professional leagues like the National Basketball Association (NBA), National Hockey League (NHL), and Major League Baseball (MLB). A "game" or a "match" in such leagues is analogous to a "round" of CoD SnD for the purposes of this paper.

We note that SnD is not the only game mode played in competitive CoD.[^4] Teams play in a head-to-head, best-of-five format, where SnD is always played as the second and fifth game modes. The best-of-five matchup could also be called a "series", but since we analyze only the SnD games, we refer to the SnD games as series.

[^4]: Hardpoint has been played as the first and fourth game modes in a matchup. The third game mode was Domination in the 2020 season, and Control in the 2021 and 2022 seasons.

## Data

The data set consists of all SnD matches played in major tournaments and qualifiers during the past three years[^5], totaling 7,792 rounds across 852 series.[^6] Data has been collected in spreadsheets by community member "IOUTurtle".[^7]

[^5]: CoD has roughly gone through three eras of professional gaming: (1) Major League Gaming (MLG) tournaments prior to 2016; (2) the CoD World League (CWL), initiated in 2016; and (3) the 12-franchise CoD League (CDL), operating since 2020. The CDL has completed three year-long "seasons" as of August 2022.

    CoD is fairly unique compared to other esports in that it runs on an annual lifecycle, with releases coming in the late fall. A new game under the same brand---Call of Duty---is published every year by a rotating set of developers. Each new game bears resemblance to past ones, often introducing relatively small variations ("improvements") to graphics, game modes, and other facets of gameplay. During the CDL era, the games released have been Modern Warfare (2020), Cold War (2021) and Vanguard (2022).

[^6]: 288 of the series occur in major tournaments, which are considered to be more competitive than the qualifiers since they are played in person (COVID-permitting), eliminating randomness due to online latency and ping. The qualifiers are played online.

[^7]: Raw data: <https://linktr.ee/CDLArchive>. Cleaned data: <https://github.com/%7Bauthor%7D/%7Brepo%7D/blob/master/data/%7Bfile%7D.csv>.

Sweeps (6-0 series) make up 4.7% of all series, while 6-5 series make up 23.7% of series. The observed offensive round win percentage across all rounds is $\bar{\tau}$ = 47.8%.[^8] Table \ref{tbl:o-win-prop-by-series-state} shows offensive round win percentages by series "state", i.e. the number of round wins by each team prior to an upcoming round. Offensive round win percentage is not quite constant, although never veers more than 10% from $\bar{\tau}$.

[^8]: Offensive round win percentage has been nearly constant across the three games during the CDL era: 1. 47.2% in MW (2020) 2. 47.9% in Cold War (2021) 3. 48.1% in Vanguard (2022)

```{=tex}
\begin{table}

\caption{Offensive round win percentage (\%) for the upcoming round, given both the offensive and defensive team's prior number of round wins. Numbers in parentheses indicate sample sizes.}

\centering
\begin{tabular}{crrrrrr}
\toprule
& \multicolumn{6}{c}{Offense round wins} \\ 
\cmidrule(lr){2-7}
Defense round wins & 0 & 1 & 2 & 3 & 4 & 5 \\ 
\midrule

0 & 47.8 (852) & 46.6 (408) & 43.1 (216) & 43.5 (115) & 43.3 (67)  & 40.5 (37)  \\
1 & 48.6 (444) & 49.3 (418) & 51.5 (309) & 43.4 (205) & 43.3 (120) & 39.4 (99)  \\
2 & 52.8 (218) & 48.9 (305) & 48.9 (315) & 46.6 (262) & 48.7 (189) & 42.1 (133) \\
3 & 54.5 (123) & 46.0 (200) & 49.6 (250) & 45.6 (248) & 44.4 (214) & 44.8 (174) \\
4 & 56.9 (65)  & 54.5 (145) & 47.2 (193) & 44.7 (228) & 55.2 (221) & 50.5 (208) \\
5 & 47.4 (38)  & 49.4 (83)  & 47.1 (136) & 50.9 (175) & 45.2 (177) & 46.0 (202) \\

\bottomrule
\end{tabular}

\label{tbl:o-win-prop-by-series-state}

\end{table}
```

The professional competition format balances the frequency with which teams play offense or defense to start a series, to the extent possible. Thus, win percentages in certain states are subjected to minimal selection bias, i.e. bias due to better teams playing defense more frequently.[^9]

[^9]: At tournaments, higher-seeded ("better") teams choose whether they want to start on offense in the SnD series, played as the second or fifth game mode of the best-of-five matchup. Their opponent is assigned to start on offense in the SnD slot not chosen. Anecdotally, the better team tends to choose to play defense in the first round of the SnD series played as the second game mode in the best-of-five matchup, although this choice is not consistent across teams, or even with the same team over time.

    Outside of tournaments, teams all play each other twice throughout the season, constituting qualifiers. Each team gets to play the higher-seeded role once in their head-to-head matchups with a given team, despite their win-loss records.

# Literature review

There have been a handful of studies of the distribution of games played in a series of a professional sport. Most assume a constant probability $\phi$ of a given team winning a game in the series, regardless of the series state. Mosteller [-@mosteller1952] observed that the American League had dominated the National League in MLB World Series matchups through 1952, implying that games should not be modeled with a constant $\phi_0 = 0.5$. Mosteller proposed three approaches for identifying the optimal constant probability value of the stronger team in the World Series, finding $\hat{\phi} \approx 0.65$ in each case: (1) solve for $\phi$ from the observed average number of games won by the loser of the series, which he called the "method of moments" approach; (2) maximize the likelihood that the sample would have been drawn from a population in which the probability of a team winning a game is constant across the series (i.e. maximum likelihood), and (3) minimize the chi-squared goodness of fit statistic $\chi^2$ as a function of $\phi$.

Chance [-@chance2020] re-examined the constant probability notion in the MLB World Series (1923--2018), the NBA Finals (1951--2018), and the NHL Stanley Cup (1939--2018). Chance found strong evidence against the null hypothesis of $\phi_0 = 0.5$ in the MLB and NHL championship series when applying Mosteller's first and second methods.

The concept of momentum goes hand-in-hand with a discussion of the nature of series outcomes.[^10] Two opposing fallacies are observed in the context of momentum: the "gambler's fallacy" (negative recency) and the "hot hand fallacy" (positive recency). Per Ayton et al. [-@ayton2004], negative recency is "the belief that, for random events, runs of a particular outcome ... will be balanced by a tendency for the opposite outcome", while positive recency is the expectation of observing future results that match recent results.

[^10]: We often use "streaks" and momentum interchangeably, but as Steeger et al. [-@steeger2021] note, momentum implies dependence between events, whereas streaking does not.

Studying both player streaks and team streaks in basketball, in both observational and controlled settings, Gilovich et al. [-@gilovich1985], henceforth GVT, did not find evidence for the hot hand phenomenon. However, Miller and Sanjurjo [-@miller2018], henceforth MS, provided a framework for quantifying streak selection bias, which effectively works in the manner posited by the gambler's fallacy. Specifically, MS stated that a "bias exists in a common measure of the conditional dependence of present outcomes on streaks of past outcomes in sequential data" implying that, under i.i.d. conditions, "the proportion of successes among the outcomes that immediately follow a streak of consecutive successes is expected to be strictly less than the underlying (conditional) probability of success". When applying streak selection bias to GVT's data, MS came to the opposite conclusions as GVT.

Other research has borrowed techniques from the field of quality control, such as identifying unlikely sequences of events with the Wald-Wolfowitz runs test (Peel and Clauset 2015, Steeger et al. 2021). Peel and Clauset found no evidence for unlikely sequences of scoring events in the NHL, College Football, and National Football League, although did in the NBA. As a check on their entropy approach to momentum identification, Steeger et al. found several NHL teams with sequences of wins in the 2018-2019 regular season that violated the Wald-Wolfowitz null hypothesis.

## Our contribution

Despite the plethora of existing research on games played in a series and momentum in sports, these topics have yet to be investigated heavily, if at all, in esports. Work has been done to examine intra-round, second-by-second win probability in other FPS titles such as Counter-Strike [@xenopoulos2022] and Valorant [@derover2021], both of which are round-based like CoD SnD. However, considering Counter-Strike and Valorant specifically, research on round-level trends seems non-existent, perhaps for one of several reasons:

1.  Both have economic aspects that can create clear advantages for one team in a round, given how prior rounds played out. CoD has no such equivalent, except for perhaps "score streaks", which infrequently occur.
2.  Teams play either offense or defense for many consecutive rounds. (The former has a 15-15-1 format and the latter uses a 12-12-1 format.) On the other hand, teams in CoD SnD rotate roles every round, analogous to a 1-1-1-1-1-1-1 format for home advantage in best-of-seven series for professional sports like the MLB, NBA, and NHL.[^11] While theoretically one might be able to account for inter-round win percentage relationships in any kind of format, such as a 5-5-1, the rotation of team sides every round is convenient for convincing ourselves that rounds could reasonably be modeled as i.i.d. Bernoulli trials.
3.  Both Counter-Strike and Valorant have overtime rules---a team must win by two rounds---which can make end-of-series sequences difficult to model properly. CoD SnD does not have overtime rules.

[^11]: 1-1-1-1-1-1-1 is not used today in these leagues, but it was at least once in each.

To our knowledge, there is no existing public statistical research on the CoD SnD format, beyond descriptive analysis on social media. While intra-round trends may be more directly applicable to teams looking for an advantage on their competition, broader investigation of a concept like the hot-hand fallacy in a sport where it has not yet been invetigated---particularly one that is less subjected to factors that may be difficult to control for, e.g. weather---should be useful as an inspiration for future researchers.

# Methodology

## Distribution of rounds played

In a best-of-$s$ format, assuming a constant round win probability $\phi$ (where $0 \leq \phi \leq 1$)[^12] for one team[^13], the expected proportion of series ending in $r$ rounds ($r \leq s$) is given by Equation \ref{eq:series-length}.

[^12]: If $\phi > 0.5$, we can say that this is the stronger team.

[^13]: $\phi$ and other symbols are selected so as to reserve symbols like $p$ for usage in other contexts without causing confusion for the reader. Upper-case symbol, e.g. $\Phi$, are consistently used in this paper to represent proportions, while lower-case symbols corresponding to upper-case symbols, e.g. $\phi$, are used to represent Bernoulli trial success rates. "Hats", e.g. $\hat{\Phi}$, are used to convey expectations, while bare symbols symbols convey observational data.

```{=tex}
\begin{equation}\label{eq:m}
m = \frac{s + 1}{2}.
\end{equation}
```
```{=tex}
\begin{equation}\label{eq:series-length}
\hat{\Phi}(r) = \frac{(r - 1)!}{(m - 1)!(s - r)!}(\phi^{m}(1 - \phi)^{r - m} + \phi^{r - m}(1 - \phi)^m).
\end{equation}
```
For example, assuming $\phi = 0.5$, the probability of a series ending in exactly nine rounds in CoD SnD, where $s=11$ (implying $m=6$), is

$$
\hat{\Phi}(9) = \frac{(9 - 1)!}{(6 - 1)!(11 - 9)!}(0.5^{6}(1 - 0.5)^{9 - 6} + 0.5^{9 - 6}(1 - 0.5)^6) = 56 (0.5^9 + 0.5^9) = 0.21875.
$$

To evaluate the constant round win probability hypothesis, we can compute the test statistic

```{=tex}
\begin{equation}\label{eq:chi-squ}
\chi^2 = \sum_{r \in R} \frac{(\Phi(r) - \hat{\Phi}(r))^2}{\hat{\Phi}(r)}
\end{equation}
```
in which $\hat{\Phi}(r)$ and $\Phi(r)$ represent the expected and observed round win proportions respectively, and $R = [6, 7, 8, 9, 10, 11]$ for CoD SnD. The null hypothesis is that the test statistic is chi-squared distributed, where the number of degrees of freedom is $\vert R \vert -1 = 5$ for Cod SnD.

## Momentum

### MS post-streak probability

Let us now consider round win proportions immediately following a streak of $k$ wins, given that the series lasts $r$ rounds. Removing our knowledge of a streak, we might model the Bernoulli round win probability as

```{=tex}
\begin{equation}\label{eq:pwr}
p_0(\text{win} | r) = p^{+|r}_0 := \begin{cases} 
  \frac{m}{r} & \text{team wins series}, \\ 
  \frac{r - m}{r} & \text{team loses series},
\end{cases}
\end{equation}
```
for $r \in R$ and $m$ from Equation \ref{eq:m}.

As implied by MS's Theorem 1, we should expect the proportion of round wins immediately following a streak of $k$ rounds wins for a series lasting $r$ rounds, $\hat{P}^{+|k,r}_{MS}$, to be strictly less than $p^{+|r}$. Although one might be tempted to mirror the equal proportions hypothesis testing performed by MS, our context is fundamentally different from that of MS, who focus on longitudinal data in controlled settings, unaffected by opposition.[^14] The number of trials is fixed in their experimental designs, but in CoD SnD, the number of rounds played is determined as a function of the max number of possible rounds ($s$) and whether or not the team wins the series. The Bernoulli trial success probability, i.e. single round win percentage in CoD SnD, is not independent of the opponent. Consequently, a statistical test of the difference in $\hat{P}^{+|k,r}$ and $\hat{P}^{-|kr}$, as performed by MS to evaluate their hypothesis regarding post-streak success rate, is not completely appropriate, although useful as a reference.

[^14]: GVT also perform statistical tests on shots from players in live games, i.e. "observational" data, but they note that their findings are likely affected by player shot selection in the face of defensive strategy by the opposing team.

### "Notional" post-streak probability

One can consider another form of the expected proportion of rounds won immediately after a streak of $k$ round wins in a best-of-$s$ series given the length of the series ($r$ rounds), the "notional" proportion $\hat{P}^{+|k,r}_0$. The Bernoulli round win probability underlying $\hat{P}^{+|k,r}_0$ is

```{=tex}
\begin{equation}\label{eq:pwkr}
p_0(\text{win} | k, r) = p^{+|k,r}_0 := \begin{cases}
  \frac{m - k}{r - k} & \text{team wins series}, \\
  \frac{s - m - k}{r - k} & \text{team loses series},
\end{cases}
\end{equation}
```
for $r \in R$ and $m$ from Equation \ref{eq:m}.

We can perform a two-tailed binomial test to evaluate the null hypothesis $\omega = \omega_0$ for the observed probability of success $\omega$ and a user-specified $\omega_0$ (where $0 \leq \omega_0 \leq 1$). If there are $r^+$ observed successes in a sample of $r$ trials and we expect that there should be $r * \omega_0$, the probability of arriving at this expected number of successes is

```{=tex}
\begin{equation}\label{eq:binom}
\binom {r}{r^+} \omega^{r^+}(1-\omega)^{r-r^+}.
\end{equation}
```
Treating the notional proportion $\hat{P}^{+|k,r}_0$ as the null $\omega_0$ and plugging in the observed proportion $P^{+|k,r}$ for $\omega$ in Equation \ref{eq:binom} (treating proportions as probabilities), we can evaluate the null hypothesis that the observed proportion of post-streak round wins is equal to the notional proportion, given streak length $k$ and series length $r$. If we can reject this null hypothesis, then we can consider team momentum, represented by post-streak success, plausible.

As a reference, we can perform the same binomial test for the MS's streak-selection-adjusted proportion, $\hat{P}^{+|k,r}_{MS}$. However, given the caveats mentioned before regarding applying MS's theorem to our setting, the results of such binomial tests should be heeded with caution.

We can further decompose Equation \ref{eq:pwkr} by the round $i$ (where $i \leq r$) in which the streak of length $k$ carries into.

```{=tex}
\begin{equation}\label{eq:pwkri}
p_0(\text{win} | k, r, i) = p^{+|k,r,i}_0 = \begin{cases}
  \frac{m - k}{r - k}, & \text{team wins series}, i \neq r, \\
  1, & \text{team wins series}, i = r, \\
  \frac{s - m - k}{r - k}, & \text{team loses series}, i \neq r, \\
  0, & \text{team loses series}, i = r.
\end{cases}
\end{equation}
```
Again, we can apply a binomial test to evaluate the hypothesis that the expected proportions, $\hat{P}^{+|k,r,i}_0$ and $\hat{P}^{+|k,r,i}_{MS}$ separately, are equal to the observed proportion $P^{+|k,r}$.

### Wald-Wolfowitz runs test

Stepping back from the (one-sided) perspective of a single team's round win probability when streaking, one can attempt to detect the hot hand phenomenon more broadly with a Wald-Wolfowitz runs test. Under the null hypothesis, the number of runs in a sequence of $r$ trials is a random variable that can take on values $+$ or $-$ and arrive at $r^+$ successes ($r^- = r - r^+$ failures), with the following mean $\mu$ and variance $\sigma^2$:

```{=tex}
\begin{equation}\label{eq:ww}
\mu = \frac{2r^{+}r^{-}}{r} + 1, \sigma^2 = \frac{(\mu-1)(\mu-2)}{r-1}.
\end{equation}
```
To incorporate our findings regarding constant round win probability, We can specify that $\Pr(r^+) = \phi$ (and, conversely, that $\Pr(r^-) = 1 - \phi$).

One can subset the observed series sequences, $\zeta(r)$, to those that violate the null hypothesis for the runs test and perform a test of equal proportions, where the null is that the observed proportion of a sequence relative to all possible sequences, $P(\zeta) = P^\zeta$, is equal to the expected proportion of the sequence relative to all possible sequences, $\hat{P}^\zeta$. The test statistic is

```{=tex}
\begin{equation}\label{eq:prop}
z = \frac{P^\zeta - \hat{P}^\zeta}{\sqrt{P^\zeta_{\delta} (1 - P^\zeta_{\delta}) (1 / N^\zeta + 1 / \hat{N}^\zeta)  } }
\end{equation}
```
where

$$
P^\zeta_{\delta} = \frac{P^\zeta - \hat{P}^\zeta}{N^\zeta - \hat{N}^\zeta}
$$

and where $N^\zeta$ and $\hat{N}^\zeta$ are the observed and expected number of sequences respectively. The null hypothesis is that $z$ is normally distributed. If $z > +z_{(1-\alpha)/2}$ or $z < -z_{(1-\alpha)/2}$ at a significance level $\alpha$, we can reject the null hypothesis for such sequences and build an argument in support of the hot hand effect.

# Results

## Distribution of rounds played

Using Equation \ref{eq:chi-squ}, we find that $\chi^2 = 16.0$ ($p$-value of 0.0068) for $\phi_0 = 0.5$. Thus, we can comfortably reject the constant probability hypothesis for the null $\phi_0 = 0.5$, even at a confidence level of $\alpha = 0.01$.

```{r}
#| label: cod_series_outcome_prop
MAX_COD_ROUND <- 11
COD_WIN_THRESHOLD <- 6
prob_of_series_lasting_r_rounds <- function(r, p = 0.5, s) {
  m <- as.integer((s + 1) / 2)
  (factorial(r - 1) / (factorial(m - 1) * factorial(r - m))) * (p^m * (1 - p)^(r - m) + p^(r - m) * (1 - p)^m)
}

expected_series_streaks_of_outcomes <- function(m, n) {
  factorial(m + n) / (factorial(m) * factorial(n))
}

# https://raw.githubusercontent.com/dgrtwo/splittestr/master/R/vectorized-prop-test.R
vectorized_prop_test_approx <- function(a, b, c, d) {
  n1 <- a + b
  n2 <- c + d
  n <- n1 + n2
  p <- (a + c) / n
  E <- cbind(p * n1, (1 - p) * n1, p * n2, (1 - p) * n2)
  
  x <- cbind(a, b, c, d)
  
  DELTA <- a / n1 - c / n2
  YATES <- pmin(.5, abs(DELTA) / sum(1 / n1 + 1 / n2))
  
  STATISTIC <- rowSums((abs(x - E) - YATES)^2 / E)
  PVAL <- pchisq(STATISTIC, 1, lower.tail = FALSE)
  PVAL
}

vectorized_prop_test_exact <- function(a, b, c, d) {
  sapply(seq_along(a), function(i) {
    fisher.test(cbind(c(a[i], c[i]), c(b[i], d[i])))$p.value
  })
}

vectorized_prop_test <- function(x1, n1, x2, n2, conf.level = 0.95) {
  a <- x1
  b <- n1 - x1
  c <- x2
  d <- n2 - x2
  
  # if any values are < 20, use Fisher's exact test
  exact <- (a < 20 | b < 20 | c < 20 | d < 20)
  
  pvalue <- rep(NA, length(a))
  
  if (any(exact)) {
    pvalue[exact] <- vectorized_prop_test_exact(a[exact], b[exact], c[exact], d[exact])
  }
  if (any(!exact)) {
    pvalue[!exact] <- vectorized_prop_test_approx(a[!exact], b[!exact], c[!exact], d[!exact])
  }
  
  mu1 <- a / (a + b)
  mu2 <- c / (c + d)
  
  ## confidence interval
  alpha2 <- (1 - conf.level) / 2
  DELTA <- mu2 - mu1
  WIDTH <- qnorm(alpha2)
  alpha <- (a + .5) / (a + b + 1)
  beta <- (c + .5) / (c + d + 1)
  
  n <- n1 + n2
  YATES <- pmin(.5, abs(DELTA) / sum(1 / n1 + 1 / n2))
  
  z <- qnorm((1 + conf.level) / 2)
  WIDTH <- z * sqrt(mu1 * (1 - mu1) / n1 + mu2 * (1 - mu2) / n2)
  
  tibble(
    estimate = DELTA,
    conf.low = pmax(DELTA - WIDTH, -1),
    conf.high = pmin(DELTA + WIDTH, 1),
    p.value = pvalue
  )
}

cod_actual_round_streaks <- cod_rounds |> 
  filter(round <= MAX_COD_ROUND) |> 
  filter(win_series) |> 
  mutate(across(win_round, as.integer)) |> 
  group_by(series_id) |> 
  summarize(
    wins = max(cumu_w),
    losses = max(cumu_l),
    ws = paste0(win_round, collapse = '-')
  ) |> 
  ungroup() |> 
  mutate(n_rounds = wins + losses) |> 
  unite(
    record, wins, losses, sep = '-'
  ) |>
  count(record, n_rounds, ws, sort = TRUE) |> 
  mutate(prop = n / sum(n))

summarize_cod_streaks <- function(p = 0.5) {
  
  expected_round_streaks <- tibble(
    n_rounds = COD_WIN_THRESHOLD:MAX_COD_ROUND
  ) |> 
    mutate(
      series_prop = map_dbl(n_rounds, ~prob_of_series_lasting_r_rounds(.x, s = MAX_COD_ROUND, p = !!p)),
      n_expected_series_streaks = map_dbl(n_rounds, ~expected_series_streaks_of_outcomes(COD_WIN_THRESHOLD, .x - COD_WIN_THRESHOLD))
    ) |> 
    transmute(
      n_rounds,
      series_prop,
      prop = series_prop / n_expected_series_streaks
    )
  
  round_streaks <- full_join(
    cod_actual_round_streaks |> 
      rename_with(~sprintf('%s_actual', .x), c(n, prop)),
    expected_round_streaks |> 
      rename_with(~sprintf('%s_expected', .x), prop),
    by = 'n_rounds'
  )
  
  round_streak_prop <- round_streaks |> 
    drop_na() |> 
    mutate(
      prop_diff = prop_actual - prop_expected,
      total_actual = sum(n_actual),
      n_expected = round(prop_expected * total_actual),
      p = vectorized_prop_test(n_actual, total_actual, n_expected, total_actual)
    ) |> 
    select(-total_actual) |> 
    unnest_wider(p) |> 
    arrange(p.value)
  
  series_outcomes <- full_join(
    cod_actual_round_streaks |> 
      group_by(record, n_rounds) |> 
      summarize(
        across(n, sum)
      ) |> 
      ungroup() |> 
      mutate(prop = n / sum(n)) |> 
      rename_with(~sprintf('%s_actual', .x), c(n, prop)),
    expected_round_streaks |> 
      select(n_rounds, prop_expected = series_prop),
    by = 'n_rounds'
  )
  
  series_outcome_prop <- series_outcomes |> 
    drop_na() |> 
    mutate(
      prop_diff = prop_actual - prop_expected,
      total_actual = sum(n_actual),
      n_expected = round(prop_expected * total_actual),
      p = vectorized_prop_test(n_actual, total_actual, n_expected, total_actual)
    ) |> 
    select(-total_actual) |> 
    unnest_wider(p) |> 
    arrange(p.value)
  
  list(
    rounds = round_streak_prop,
    series = series_outcome_prop
  )
}

cod_streaks_naive_res <- summarize_cod_streaks(p = 0.5)
```

```{r}
#| label: cod_series_outcomes_naive_chi
generate_chi_label <- function(statistic, p.value) {
  sprintf(
    '%.1f (%s)', 
    statistic, 
    ifelse(p.value <= 0.01, '<=0.01', as.character(round(p.value, 2)))
  )
}

perform_series_chi_test <- function(series) {
  chisq.test(
    series$n_actual, 
    p = series$prop_expected
  ) |> 
    broom::tidy()
}

cod_series_outcomes_naive_chi <- cod_streaks_naive_res$series |> 
  perform_series_chi_test()
```

```{r}
#| label: cod-alternative-to-p=0.5-method-prep
min_p1 <- 0.5
max_p1 <- 0.7
interval_p1 <- 0.0025
cod_ps <- tibble(p = seq(min_p1, max_p1, by = 0.0025))
```

```{r}
#| label: cod-alternative-to-p=0.5-method-1
## p. 365 on https://math.mit.edu/classes/18.095/2016IAP/lec9/Sports_Mosteller1952_WorldSeries.pdf
cod_prob_of_series_lasting_r_rounds <- function(r, p) {
  prob_of_series_lasting_r_rounds(r = r, p = p, s = 11)
}

theoretical_cod_series_length <- function(p) {
  6 * cod_prob_of_series_lasting_r_rounds(r = 6, p = p) +
    7 * cod_prob_of_series_lasting_r_rounds(r = 7, p = p) +
    8 * cod_prob_of_series_lasting_r_rounds(r = 8, p = p) +
    9 * cod_prob_of_series_lasting_r_rounds(r = 9, p = p) +
    10 * cod_prob_of_series_lasting_r_rounds(r = 10, p = p) +
    11 * cod_prob_of_series_lasting_r_rounds(r = 11, p = p)
}

cod_rounds_per_series <- cod_streaks_naive_res$series |>
  summarize(
    actual = sum(n_rounds * prop_actual),
    expected = sum(n_rounds * prop_expected)
  )

cod_theoretical_series_lengths1 <- cod_ps |>
  mutate(
    theoretical_series_length = map_dbl(p, theoretical_cod_series_length),
    diff = theoretical_series_length - cod_rounds_per_series$actual
  )

cod_theoretical_p1 <- cod_theoretical_series_lengths1 |>
  slice_min(abs(diff), n = 1) |>
  pull(p)
```

```{r}
#| label: cod-alternative-to-p=0.5-method-2
pluck_cod_n <- function(.n_rounds) {
  cod_streaks_naive_res$series |>
    filter(n_rounds == .n_rounds) |>
    pull(n_actual)
}

cod_n6 <- pluck_cod_n(6)
cod_n7 <- pluck_cod_n(7)
cod_n8 <- pluck_cod_n(8)
cod_n9 <- pluck_cod_n(9)
cod_n10 <- pluck_cod_n(10)
cod_n11 <- pluck_cod_n(11)

## This adjustment is something that is not done in the paper. We do it here to reduce
##   the magnitude of the exponents.
cod_pm <- pmin(cod_n6, cod_n7, cod_n8, cod_n9, cod_n10, cod_n11)
cod_n6adj <- round(10 * cod_n6 / cod_pm)
cod_n7adj <- round(10 * cod_n7 / cod_pm)
cod_n8adj <- round(10 * cod_n8 / cod_pm)
cod_n9adj <- round(10 * cod_n9 / cod_pm)
cod_n10adj <- round(10 * cod_n10 / cod_pm)
cod_n11adj <- round(10 * cod_n11 / cod_pm)

maximize_cod_series_p <- function(p) {
  cod_prob_of_series_lasting_r_rounds(r = 6, p = p)^cod_n6adj *
    cod_prob_of_series_lasting_r_rounds(r = 7, p = p)^cod_n7adj *
    cod_prob_of_series_lasting_r_rounds(r = 8, p = p)^cod_n8adj *
    cod_prob_of_series_lasting_r_rounds(r = 9, p = p)^cod_n9adj *
    cod_prob_of_series_lasting_r_rounds(r = 10, p = p)^cod_n10adj *
    cod_prob_of_series_lasting_r_rounds(r = 11, p = p)^cod_n11adj
}

cod_theoretical_series_lengths2 <- cod_ps |>
  mutate(
    theoretical_series_length = map_dbl(p, maximize_cod_series_p)
  )

cod_theoretical_p2 <- cod_theoretical_series_lengths2 |>
  slice_max(theoretical_series_length, n = 1) |>
  pull(p)
```

```{r}
#| label: cod-alternative-to-p=0.5-method-3
summarize_cod_series_chi <- function(p) {
  summarize_cod_streaks(p) |> 
    pluck('series') |> 
    perform_series_chi_test()
}

cod_theoretical_series_lengths3 <- cod_ps |> 
  filter(p >= 0.56, p < 0.585) |> 
  mutate(
    chi_squ = map_dbl(
      p,
      ~summarize_cod_series_chi(.x) |> 
        pluck('statistic')
    )
  )

cod_theoretical_p3 <- cod_theoretical_series_lengths3 |> 
  slice_min(chi_squ, n = 1) |> 
  pull(p)
```

```{r}
#| label: cod-alternatives-to-p=0.5
cod_streaks_other_res <- tibble(
  p = c(0.5, cod_theoretical_p1, cod_theoretical_p2, cod_theoretical_p3),
  idx = 0L:3L,
  name = c('0. Naive', '1. Method of moments', '2. Maximum likelihood', '3. Minimum Chi-square')
) |> 
  mutate(
    series_streaks = map(
      p,
      ~summarize_cod_streaks(.x) |> 
        pluck('series') 
    ),
    chi_squ = map(
      series_streaks,
      ~.x |> 
        perform_series_chi_test() |> 
        select(statistic, p.value)
    )
  )
```

Table \ref{tbl:prob-series-lasts-r-rounds} shows the expected series lasting $r$ rounds, the expected proportion of series given $\phi_0 = 0.5$, $\hat{\Phi}_0(r)$, and the observed proportions, $\Phi(r)$.

```{=tex}
\begin{table}
\caption{The expected proportion of CoD SnD series lasting $r$ rounds, $\hat{\Phi}_0(r)$, under the assumption that each team has a constant round win probability $\phi_0 = 0.5$. Additionally, the observed frequencies for CoD SnD shown as a count $N(r)$ and as a proportion $\Phi(r)$ of all series ($\sum_{r \in R} N(r)$, where $r \in R = [6, 7, 8, 9, 10, 11]$).}

\centering
\begin{tabular}{rrrr}
\toprule
$r$ & $N(r)$ & $\Phi(r)$ & $\hat{\Phi}_0(r)$ \\ 
\midrule

6 & 40 & $4.7\%$ & $3.1\%$ \\ 
7 & 101 & $11.9\%$ & $9.4\%$ \\ 
8 & 141 & $16.5\%$ & $16.4\%$ \\ 
9 & 185 & $21.7\%$ & $21.9\%$ \\ 
10 & 183 & $21.5\%$ & $24.6\%$ \\ 
11 & 202 & $23.7\%$ & $24.6\%$ \\ 

\bottomrule
\end{tabular}

\label{tbl:prob-series-lasts-r-rounds}

\end{table}
```
Table \ref{tbl:mosteller-methods-results} shows the alternate values for the constant round win probability that we find when applying the three methods suggested by Mosteller [-@mosteller1952]. Each is approximately equal to 0.575. When applying Equation \ref{eq:chi-squ}, each results in a $\chi^2$ value for which we cannot reject the constant probability null hypothesis. Note that rejection of the null hypothesis is the baseline and failure to reject the null is the non-trivial result, contrary to most statistical analysis.

```{=tex}
\begin{table}

\caption{Alternate estimates of the constant probability ($\phi$) for winning a given round in a CoD SnD, applying the three methods suggested by Mosteller (1952), in addition to the naive ($\phi_0 = 0.5$).}

\centering
\begin{tabular}{lrr}
\toprule
Method & $\phi$ & $\chi^2$ ($p$-value) \\
\midrule

0. Naive & 0.5000 & 16.0 ($\leq$ 0.01) \\
1. Method of moments & 0.5725 & 3.6 (0.6) \\
2. Maximum likelihood & 0.5750 & 3.5 (0.62) \\
3. Minimum ($\chi^2$) & 0.5775 & 3.5 (0.62) \\

\bottomrule
\end{tabular}

\label{tbl:mosteller-methods-results}

\end{table}
```
Table \ref{tbl:alternative-constant-ps} shows the new $\hat{\Phi}(r)$ when re-applying Equation \ref{eq:series-length} for the maximum likelihood estimate $\phi_2 = 0.575$, resulting in a new set of expected proportions of series lasting $r$ rounds $\hat{\Phi}_2(r)$.[^15] We observe that $\hat{\Phi}_2(r)$ is larger than $\hat{\Phi}_0(r)$ for $r \in [6, 7]$, more closely matching $\Phi(r)$. $\hat{\Phi}_2(r)$ is also closer to the observed $\Phi(r)$ for $r \in [9, 10]$, although not for $r \in [8, 11]$.

[^15]: The method of moments and minimum $\chi^2$ estimates for $\phi$ are omitted simply because the results would be nearly identical to those for the maximum likelihood estimate of $\phi$ (since they are all $\approx 0.575$).

```{=tex}
\begin{table}
\caption{The observed proportion of CoD SnD series, $\Phi(r)$, ending in $r$ rounds, compared to the expected proportion, $\hat{\Phi}_0(r)$, under the naive assumption $\phi_0 = 0.5$ and the expected proportion, $\hat{\Phi}_2(r)$, under the maximum likelihood estimate, $\phi_2 = 0.575$, for constant round win probability.}

\centering
\begin{tabular}{rrrrrr}
\toprule
$r$ & $\Phi(r)$ & $\hat{\Phi}_0(r)$ = 0.5 & $\hat{\Phi}_2(r)$ = 0.575 \\
\midrule

6 & 4.7\% & 3.1\% & 4.2\% \\
7 & 11.9\% & 9.4\% & 11.2\% \\
8 & 16.5\% & 16.4\% & 17.8\% \\
9 & 21.7\% & 21.9\% & 21.8\% \\
10 & 21.5\% & 24.6\% & 23.0\% \\
11 & 23.7\% & 24.6\% & 22.0\% \\

\bottomrule
\end{tabular}

\label{tbl:alternative-constant-ps}

\end{table}
```
Observing that the null hypothesis acceptance shown in Table \ref{tbl:mosteller-methods-results}, along with the observation that $\hat{\Phi}_2(r)$ reasonably matches $\Phi(r)$ (in comparison to $\hat{\Phi}_0(r)$), we can say that the constant round win probability assumption is valid in CoD SnD series with the appropriate choice of $\phi$ ($\approx 0.575$).

## Momentum

### Post-streak probability

```{r}
#| label: cod_round_win_prop_after_k_wins
## http://keyonvafa.com/hot-hand/
get_post_streak_prob <- function(n, k, p = 0.5) {
  tosses <- rbinom(n, 1, p)
  runs <- rle(tosses)
  n_neg_after <- length(which(runs$values == 1 & runs$lengths >= k))
  n_pos_after <- sum(runs$lengths[which(runs$values == 1 & runs$lengths >= k)] - k)
  
  ## edge case
  if (n %in% cumsum(runs$lengths)[which(runs$values == 1 & runs$lengths >= k)]) {
    n_neg_after <- n_neg_after - 1
  }
  
  n_pos_after / (n_pos_after + n_neg_after)
}

simulate_post_streak_prob <- function(sims = 10000, seed = 42, ...) {
  withr::local_seed(seed)
  rerun(
    sims,
    get_post_streak_prob(...)
  ) |> 
    flatten_dbl() |> 
    mean(na.rm = TRUE)
}

vectorized_binom_test <- function(x, n, p, ...) {
  map_dfr(
    seq_along(x), 
    function(i) {
      res <- binom.test(x[i], n[i], p = p[i], ...)
      tibble(
        estimate = res$estimate,
        conf.low = res$conf.int[[1]],
        conf.high = res$conf.int[[2]],
        p.value = res$p.value
      )
    }
  )
}

cod_round_streaks <- cod_rounds |> 
  group_by(series_id, team) |> 
  mutate(
    won_prior_round1 = lag(win_round, n = 1, default = NA),
    won_prior_round2 = lag(win_round, n = 2, default = NA),
    won_prior_round3 = lag(win_round, n = 3, default = NA),
    won_prior_round4 = lag(win_round, n = 4, default = NA),
    won_prior_round5 = lag(win_round, n = 5, default = NA)
  ) |> 
  ungroup() |>
  select(
    series_id,
    team,
    is_offense,
    round,
    n_rounds,
    win_series,
    win_round,
    starts_with('won_prior_round')
  )

postprocess_round_streaks <- function(k, probs_nx_k, ...) {
  
  cols <- sprintf('won_prior_round%d', 2:k)
  col_syms <- syms(cols)
  cod_round_streaks_after_x <- cod_round_streaks |> 
    drop_na(!!!col_syms) |> 
    count(n_rounds, win_series, ..., win_round, won_prior_round1, !!!col_syms, sort = TRUE)
  
  cod_round_win_prop_after_z <- cod_round_streaks_after_x |> 
    filter(won_prior_round1, !!!col_syms)
  
  suppressMessages(
    cod_round_win_prop_after_z_with_probs <- cod_round_win_prop_after_z |> 
      group_by(win_series, n_rounds, ...) |> 
      mutate(
        total = sum(n),
        prop = n / total
      ) |> 
      ungroup() |> 
      select(n_rounds, win_round, win_series, ..., n, total, prop) |> 
      # filter(!win_round) |> 
      arrange(n_rounds, win_round, ...) |> 
      inner_join(
        probs_nx_k
      )
  )
  
  cod_round_win_prop_after_z_with_probs |> 
    mutate(
      notional_p_value = vectorized_binom_test(n, total, p = notional_prop)$p.value,
      ms_p_value = vectorized_binom_test(n, total, p = ms_prop)$p.value
    )
}

compute_probs_nx_k <- function(df, k) {
  df |> 
    mutate(
      is_valid = case_when(
        win_series ~ TRUE,
        (n_rounds - !!k - 6L) >= 0L ~ TRUE,
        TRUE ~ FALSE
      )
    ) |> 
    filter(is_valid) |> 
    select(-is_valid) |> 
    mutate(
      naive_prop = ifelse(win_series, 6L / n_rounds, (n_rounds - 6L) / n_rounds),
      notional_prop = (ifelse(win_series, 6L, n_rounds - 6L) - k) / (n_rounds - k),
      across(c(naive_prop, notional_prop), ~ifelse(win_round, .x, 1 - .x)),
      ms_prop = map2_dbl(
        n_rounds, naive_prop, 
        ~simulate_post_streak_prob(n = ..1, k = !!k, p = ..2)
      ),
      across(
        ms_prop,
        ~ifelse(win_round, .x, naive_prop + (naive_prop - .x))
      ),
      across(
        ms_prop,
        ~ifelse(is.nan(ms_prop), naive_prop, .x)
      )
    )
}

summarize_cod_win_prop_after_k_wins <- function(k) {
  probs_nx_k <- crossing(
    n_rounds = 7L:11L,
    win_series = c(TRUE, FALSE),
    win_round = c(TRUE, FALSE)
  ) |> 
    compute_probs_nx_k(k)
  
  postprocess_round_streaks(
    k = k,
    probs_nx_k = probs_nx_k
  )
}

cod_round_win_prop_after_k_wins <- tibble(win_streak = 2L:5L) |>
  mutate(
    data = map(win_streak, summarize_cod_win_prop_after_k_wins)
  ) |>
  unnest(data)
cod_round_win_prop_after_k_wins |> filter(win_round, notional_p_value < 0.05)
cod_round_win_prop_after_k_wins |> filter(win_round, ms_p_value < 0.05)
```

Given that people typically perceive streaks as beginning after the third success (or failure) at minimum [@carlson2007], we focus on streaks of three round wins.[^16] Table \ref{tbl:pwkr} compares the notional and MS expected proportions, $\hat{P}^{+|k,r}_0$ and $\hat{P}^{+|k,r}_{MS}$ respectively, with the observed round win proportion, $P^{+|kr}$, following streaks of $k=3$ round wins given the series outcome.

[^16]: Three happens to also be a reasonable number for series that last at maximum 11 rounds.

```{=tex}
\begin{table}

\caption{Given the round win streak $k=3$, the length of the series ($r$ rounds), and the series winner, the observed count of rounds wins, $r^{+|k=3,r}$, and proportion of round wins, $P^{+|k=3,r}$, among $N^{k=3,r}$ instances where a team could win after the streak ($P^{+|k=3,r} = \frac{r^{+|k=3,r}}{N^{k=3,r}}$). Additionally, the notional and MS expected proportions, $\hat{P}^{+|k=3,r}_0$ and $\hat{P}^{+|k=3,r}_{MS}$ respectively.}

\centering
\begin{tabular}{rcrrrrr}
\toprule
$r$ & \text{Win series?} & $r^{+|k=3,r}$ & $N^{k=3,r}$ & $P^{+|k=3,r}$ & $\hat{P}^{+|k=3,r}_0$ & $\hat{P}^{+|=k=3,r}_{MS}$ \\ 
\midrule

7 & yes & 156 & 209 & 74.6\% & 75.0\% & 75.7\% \\ 
8 & yes & 130 & 209 & 62.2\% & 60.0\% & 61.9\% \\ 
9 & yes & 100 & 193 & 51.8\% & 50.0\% & 52.7\% \\ 
10 & no & 8 & 60 & 13.3\% & 14.3\% & 26.7\% \\ 
10 & yes & 66 & 151 & 43.7\% & 42.9\% & 44.7\% \\ 
11 & no & 31 & 129 & 24.0\% & 25.0\% & 30.8\% \\ 
11 & yes & 60 & 150 & 40.0\% & 37.5\% & 38.8\% \\ 

\bottomrule
\end{tabular}

\label{tbl:pwkr}

\end{table}
```
With the exception of $\hat{P}^{+|k,r}_{MS}$ when $r = 10$ and the eventual series loser is the team that wins after a streak of three round wins, all un-adjusted binomial test $p$-values are greater than the $\alpha = 0.05$ confidence level, implying that we cannot reject the null hypothesis that the expected notional and MS post-streak round win proportions are different than the observed proportion. However, after adjusting the $p$-values with the Benjamini and Yekutieli (BY) correction [-@benjamini2001], the null hypothesis cannot be rejected for any case.[^17]

[^17]: As noted before, the MS expectations may be unreliable for CoD SnD, so one is inclined to prefer the results of the notional binomial tests.

When performing the same tests for streaks of two, four, and five, there is no case in which we can reject the binomial null hypothesis for the expected notional proportion $\hat{P}^{+|k,r}_0$ (even before the BY p-value correction). The null hypothesis can only be rejected for the expected MS proportion $\hat{P}^{+|k,r}_{MS}$ when $k = 5, r = 7$ after applying the BY p-value correction.[^18]

[^18]: At 36, the sample size for $k = 5, r = 7$ is not as large as most other combinations of $k$ and $r$.

```{r}
#| label: cod_round_win_prop_after_k_wins_i_rounds
summarize_cod_win_prop_after_k_wins_i_rounds <- function(k) {
  probs_nx_k <- crossing(
    n_rounds = 7L:11L,
    round = 3L:11L,
    win_series = c(TRUE, FALSE),
    win_round = c(TRUE, FALSE)
  ) |> 
    filter(round >= (!!k + 1L), n_rounds > round) |> 
    compute_probs_nx_k(k)
  
  postprocess_round_streaks(
    k = k,
    probs_nx_k = probs_nx_k,
    round
  )
}

init_cod_round_win_prop_after_k_wins_i_rounds <- tibble(win_streak = 3L) |>
  mutate(
    data = map(win_streak, summarize_cod_win_prop_after_k_wins_i_rounds)
  ) |>
  unnest(data) |> 
  select(-starts_with('ms'))

cod_round_win_prop_after_k_wins_i_rounds <- init_cod_round_win_prop_after_k_wins_i_rounds |>
  group_by(win_streak, win_round, n_rounds, round) |> 
  mutate(
    total = sum(n),
    prop = n / total
  ) |> 
  ungroup() |> 
  mutate(
    notional_p_value = vectorized_binom_test(n, total, p = notional_prop)$p.value |> round(2)
  )

# ## example of why we needed to recalculate prop... prop and notional_prop should sum to 1, grouped by win_streak and win_round
# cod_round_win_prop_after_k_wins_i_rounds |> 
#   filter(win_streak == 3, n_rounds == 11, round == 10)
# 
# ## 5-3?
# cod_round_win_prop_after_k_wins_i_rounds |> 
#   filter(win_streak == 2) |> 
#   filter(n_rounds == 11, round == 11)
```

Now let us explicitly consider the round, $i$. As shown in Table \ref{tbl:pw3ri}, the notional win proportion in the round $i$ immediately following a streak of $k$ round wins in a series lasting $r$ rounds, $\hat{P}{+|k,r,i}_0$, is statistically different than the observed $P^{+|k,r,i}$ in several cases, notably when $r = i$ for each of $r \in [9, 10, 11]$.[^19] Interestingly, the observed rate is greater than the expected rate in each case where we can reject the null hypothesis, whereas the observed rate is less than the expected rate in all other cases, as well in a strong majority of cases not shown.

[^19]: The null can also be rejected for $r \in [7, 8]$, although these are not shown, and the sample sizes are smaller.

```{=tex}
\begin{table}

\caption{Given the round win streak $k=3$, the index of the round immediately following the streak $i$, the length of the series ($r$ rounds), and the series winner, the observed count of rounds wins, $r^{+|k=3,r,i}$, and proportion of round wins, $P^{+|k=3,r,i}$, among $N^{k=3,r,i}$ instances where a team could win after the streak. Additionally, the notional proportion, $\hat{P}^{+|k=3,r,i}_0$. Table restricted to $r \in [10, 11]$, $i < r$, and $N^{k=3,r,i} \geq 10$ for brevity.}

\centering
\begin{tabular}{rrcrrrr}
\toprule
$r$ & $i$ & \text{Win series?} & $r^{+|k=3,r,i}$ & $N^{k=3,r,i}$ & $P^{+|k=3,r,i}$ & $\hat{P}^{+|k=3,r,i}_0$\\ 
\midrule

10 & 7 & no & 2 & 13 & 15.4\% & 14.3\% \\ 
10 & 7 & yes & 11 & 13 & 84.6\% & 42.9\% \\ 
10 & 8 & no & 1 & 10 & 10.0\% & 14.3\% \\ 
10 & 8 & yes & 9 & 10 & 90.0\% & 42.9\% \\ 
11 & 5 & no & 9 & 13 & 69.2\% & 25.0\% \\ 
11 & 5 & yes & 4 & 13 & 30.8\% & 37.5\% \\ 
11 & 6 & no & 4 & 13 & 30.8\% & 25.0\% \\ 
11 & 6 & yes & 9 & 13 & 69.2\% & 37.5\% \\ 
11 & 7 & no & 3 & 10 & 30.0\% & 25.0\% \\ 
11 & 7 & yes & 7 & 10 & 70.0\% & 37.5\% \\ 
11 & 9 & no & 5 & 15 & 33.3\% & 25.0\% \\ 
11 & 9 & yes & 10 & 15 & 66.7\% & 37.5\% \\ 
11 & 10 & no & 5 & 10 & 50.0\% & 25.0\% \\ 
11 & 10 & yes & 5 & 10 & 50.0\% & 37.5\% \\ 

\bottomrule
\end{tabular}

\label{tbl:pw3ri}

\end{table}
```
```{r}
#| label: cod_round_arrangements
simulate_cod_round_arrangements <- function(p = 0.5, n_sims = 10000, seed = 42) {
  withr::local_seed(seed)
  w <- sample(c(0, 1), size = MAX_COD_ROUND * n_sims, replace = TRUE, prob = c(1 - p, p))
  m <- matrix(w, nrow = n_sims, ncol = MAX_COD_ROUND)
  df <- as_tibble(m)
  names(df) <- sprintf('%d', 1:MAX_COD_ROUND)
  df$i <- 1:nrow(df)
  
  sim_rounds <- tibble(
    i = rep(1:n_sims, each = MAX_COD_ROUND),
    r = rep(1:n_sims, times = MAX_COD_ROUND),
    w = w
  ) |> 
    group_by(i) |> 
    mutate(
      cumu_w = cumsum(w),
      cumu_l = cumsum(w == 0)
    ) |> 
    ungroup() |> 
    mutate(
      cumu_wl_max = ifelse(cumu_w > cumu_l, cumu_w, cumu_l)
    ) |> 
    filter(cumu_wl_max <= COD_WIN_THRESHOLD)
  
  sim_rounds <- df |> 
    pivot_longer(
      -i,
      names_to = 'r',
      values_to = 'w'
    ) |> 
    mutate(
      across(r, as.integer)
    ) |> 
    group_by(i) |> 
    mutate(
      cumu_w = cumsum(w),
      cumu_l = cumsum(w == 0)
    ) |> 
    ungroup() |> 
    mutate(
      cumu_wl_max = ifelse(cumu_w > cumu_l, cumu_w, cumu_l)
    ) |> 
    filter(cumu_wl_max <= COD_WIN_THRESHOLD)
  
  sim_rounds_to_drop <- anti_join(
    sim_rounds |> 
      filter(cumu_wl_max == COD_WIN_THRESHOLD),
    sim_rounds |> 
      filter(cumu_wl_max == COD_WIN_THRESHOLD) |> 
      group_by(i) |> 
      slice_min(r, n = 1) |> 
      ungroup(), 
    by = c('i', 'r', 'w', 'cumu_w', 'cumu_l', 'cumu_wl_max')
  )
  
  sim_rounds <- sim_rounds |> 
    anti_join(
      sim_rounds_to_drop, 
      by = c('i', 'r', 'w', 'cumu_w', 'cumu_l', 'cumu_wl_max')
    )
  
  ## always from offensive perspective
  pre_expected_records <- sim_rounds |> 
    inner_join(
      sim_rounds |> 
        filter(cumu_wl_max == COD_WIN_THRESHOLD) |> 
        mutate(
          zeros_win = cumu_l == cumu_wl_max
        ) |> 
        select(i, zeros_win),
      by = 'i'
    ) |> 
    mutate(
      across(w, ~ifelse(zeros_win, abs(1 - .x), .x))
      # across(w, ~ifelse(zeros_win, '-', '+'))
    )
  
  pre_expected_records |> 
    group_by(i) |> 
    summarize(
      cumu_w = max(cumu_w),
      cumu_l = max(cumu_l),
      ws = paste0(w, collapse = ' ')
    ) |> 
    ungroup() |> 
    mutate(
      wins = ifelse(cumu_w == COD_WIN_THRESHOLD, cumu_w, cumu_l),
      losses = ifelse(cumu_w == COD_WIN_THRESHOLD, cumu_l, cumu_w),
      n_rounds = wins + losses
    ) |> 
    count(n_rounds, ws, sort = TRUE) |> 
    mutate(prop = n / sum(n))
}

cod_expected_round_arrangements_naive <- simulate_cod_round_arrangements(0.5)
cod_expected_round_arrangements_mle <- simulate_cod_round_arrangements(0.575)

cod_actual_round_arrangements <- cod_rounds |> 
  filter(win_series) |> 
  mutate(across(win_round, as.integer)) |> 
  group_by(year, event, series) |> 
  summarize(
    wins = max(cumu_w),
    losses = max(cumu_l),
    ws = paste0(win_round, collapse = '-')
  ) |> 
  ungroup() |> 
  mutate(
    n_rounds = wins + losses
  ) |> 
  count(n_rounds,  ws, sort = TRUE) |> 
  mutate(prop = n / sum(n))

perform_run_tests <- function(cod_expected_round_arrangements) {
  suppressWarnings(
    cod_run_tests <- cod_expected_round_arrangements |> 
      select(n_rounds, ws) |> 
      separate(ws, into = as.character(1:11), remove = FALSE) |> 
      pivot_longer(
        -c(n_rounds, ws),
        names_to = 'round',
        values_to = 'w',
        values_drop_na = TRUE
      ) |> 
      mutate(
        across(c(round, w), as.integer)
      ) |> 
      select(-round) |> 
      group_by(n_rounds, ws) |> 
      summarize(
        w = list(w)
      ) |> 
      ungroup() |> 
      mutate(
        p_value = map_dbl(w, ~randtests::runs.test(.x, threshold = 0.5)$p.value),
        is_significant = p_value <= 0.05
      ) |> 
      select(n_rounds, ws, p_value, is_significant)
  )
  
  expected_total <- sum(cod_expected_round_arrangements$n)
  actual_total <- sum(cod_actual_round_arrangements$n)
  cod_round_arrangements <- cod_expected_round_arrangements |> 
    rename(n_expected = n, prop_expected = prop) |>
    mutate(
      n_expected_adj = n_expected * (!!actual_total) / (!!expected_total)
    ) |> 
    left_join(
      cod_actual_round_arrangements |> 
        rename(n_actual = n, prop_actual = prop),
      by = c('n_rounds', 'ws')
    ) |> 
    inner_join(
      cod_run_tests,
      by = c('n_rounds', 'ws')
    ) |> 
    mutate(
      across(n_actual, replace_na, 0L),
      across(prop_actual, replace_na, 0)
    )
}

cod_round_arrangements_naive <- cod_expected_round_arrangements_naive |> perform_run_tests()
cod_round_arrangements_mle <- cod_expected_round_arrangements_mle |> perform_run_tests()

cod_round_arrangements_mle |> 
  filter(!is.na(p_value)) |> 
  filter(is_significant) |> 
  mutate(
    total_actual = sum(n_actual),
    total_expected_adj = sum(n_expected_adj)
  ) |>
  mutate(
    prop_p_value = vectorized_prop_test(
      n_actual, 
      total_actual, 
      round(n_expected_adj), 
      round(total_expected_adj)
    )$p.value,
    orig_prop_p_value = prop_p_value,
    across(prop_p_value, ~p.adjust(.x, method = 'BY') |> round(3))
  )
```

### Wald-Wolfowitz runs test

In Table \ref{tbl:ww-sequences}, the observed and expected count of sequences, $N^{\zeta r}$ and $\hat{N}^{\zeta r}$ respectively, are shown for sequences, $\zeta(r)$, for which one can reject the Wald-Wolfowitz null hypothesis, given probability of Bernoulli trial success $\phi = 0.575$,[^20] at a confidence level of $\alpha = 0.05$, for $r \inf [8, 9, 10]$. In addition to the 13 sequences shown, there are 12 additional sequences for $r = 11$.

[^20]: If we used $\phi = 0.5$, we would get the same exact sequences.

```{=tex}
\begin{table}

\caption{Sequences ($\zeta(r)$) for which we can reject Wald-Wolfowitz null hypothesis for $r \in [8, 9, 10]$, where round wins and losses are denoted with $+$ and $-$ respectively. The observed count, $N^{\zeta r}$, and proportion, $P^{\zeta r}$, of all possible sequences, as well as the expected count, $\hat{N}^{\zeta r}$, and proportion, $\hat{P}^{\zeta r}$. $\hat{N}^{\zeta r}$ is scaled to the observed number of series played, hence its non-integer value.}

\centering
\begin{tabular}{rlrrrr}
\toprule
$r$ & $\zeta(r)$ & $N^{\zeta r}$ & $P^{\zeta r}$ & $\hat{N}^{\zeta r}$ & $\hat{P}^{\zeta r}$ \\ 
\midrule
8 & - - + + + + + + & 4 & 0.47\% & 7.67 & 0.90\% \\ 
9 & - - - + + + + + + & 4 & 0.47\% & 3.75 & 0.44\% \\ 
10 & + + + + - - - - + + & 1 & 0.12\% & 2.39 & 0.28\% \\ 
10 & + - - - - + + + + + & 0 & 0.00\% & 2.04 & 0.24\% \\ 
10 & + - + - + + - + - + & 3 & 0.35\% & 2.04 & 0.24\% \\ 
10 & + + + - - - - + + + & 2 & 0.23\% & 2.04 & 0.24\% \\ 
10 & - - - - + + + + + + & 2 & 0.23\% & 1.79 & 0.21\% \\ 
10 & + - + - + - + + - + & 3 & 0.35\% & 1.70 & 0.20\% \\ 
10 & + - + + - + - + - + & 0 & 0.00\% & 1.70 & 0.20\% \\ 
10 & + + - - - - + + + + & 1 & 0.12\% & 1.19 & 0.14\% \\ 
10 & + - + - + - + - + + & 3 & 0.35\% & 1.11 & 0.13\% \\ 
10 & + + - + - + - + - + & 0 & 0.00\% & 1.11 & 0.13\% \\ 
10 & + + + + + - - - - + & 2 & 0.23\% & 1.02 & 0.12\% \\ 

\bottomrule
\end{tabular}

\label{tbl:ww-sequences}
\end{table}
```
The relative frequency of the expected proportions are based on 10,000 simulations using the constant round probability $\phi_2 = 0.575$. A test for the difference between the observed and expected proportions of all sequences, $P^{\zeta r}$ and $\hat{P}^{\zeta r}$ respectively, indicates that the null hypothesis---that the two proportions are equal---cannot be rejected for any of the 25 significant sequences.

# Discussion

Anecdotally, most observers swear by the existence of momentum in CoD SnD series, to the extent that vernacular has been developed to describe such phenomenon. Viewers have come to embrace the "5-3" phenomenon, where teams win three consecutive rounds after facing a 5-3 deficit to win 6-5. There is even a term for the rare 0-5 comeback---a "full sail".

However, our results do not provide evidence for momentum on several fronts: (1) the evidence supporting the constant round probability; (2) the failure to reject the binomial null hypothesis for post-streak win proportions; and (3) the failure to reject the equal proportions null hypothesis for streaky round sequences identified by the Wald-Wolfowitz runs test.

Perhaps it is not surprising that we did not find evidence in favor of the hot hand effect given the small "skill gap" in the CoD relative to other esports. (Most professional esports players who have played CoD, including CoD players themselves, would not hesitate to state this.) A small skill gap fosters randomness in outcomes, implying that a given team is less likely to enjoy streaks of success in SnD.

For the sake of brevity, we did not account for offensive or defensive role when considering win proportions in the rounds immediately following streaks. For example, it would be interesting to look at whether the round win percentage is higher after a streaks of three round wins when the team starts the streak as the offensive team. This would mean that, in the round immediately following the last streak win, the streaking team would be playing defense, where teams are slightly more likely to win on average. On the other hand, streaks of three starting with an offensive round win are less likely to occur than such streaks starting with a defensive win-$\bar{\tau}^2 (1 - \bar{\tau}) \approx$ 11.9% in the former case and $\bar{\tau} (1 - \bar{\tau})^2 \approx$ 13.0% in the latter case---so we would need to properly account for sample size differences.

In the future, we could account for the quality of the teams, both in general and on specific maps. Even if doing so does not change the results, we could gain additional insight into why we do not observe statistically significant streakiness.

Further regarding future work, we have contacted the Twitter user "R11stats", who privately tracks in-round player engagements. R11stats expressed intent on making the data public, which would allow for research into player-specific momentum. While it seems there is no team-level hot hand effect in CoD SnD, perhaps there is with eliminations performed by individual players.

# References {.unnumbered}

::: {#refs}
:::
