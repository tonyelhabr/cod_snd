---
title: "They're Clutching up! Team Momentum in Round-Based Esports"
authors:
  - name: Tony ElHabr
    affiliations:
      - name: Georgia Institute of Technology
    email: anthonyelhabr@gmail.com
    url: https://tonyelhabr.rbind.io
abstract: |
  My research investigates patterns in round win percentages in professional Search and Destroy (SnD) matches of the popular first-person shooter game Call of Duty (CoD). First, I find evidence supprting the hypothesis that round win probability can be modeled as a constant across the series, although not at the naive 50%. Second, I examine post-streak round win probability, given the series outcome. I find no evidence that post-streak win rate is significantly different than expected for combinations of streak length, series length, and series winner, suggesting that the perception of momentum at the team level is deceiving.
bibliography: references.bib  
output:
  rticles::arxiv_article:
    includes:
      in_header: preamble.tex
---

```{r}
#| label: setup
#| include: false
#| echo: false
#| eval: true
knitr::opts_chunk$set(
  eval = FALSE,
  echo = FALSE,
  include = FALSE,
  message = FALSE,
  warning = FALSE
)
```

```{r}
#| label: setup-code
library(dplyr)
library(qs)
library(tidyr)
library(purrr)
library(gt)
library(scales)

cod_rounds <- qs::qread('data/cod_rounds.qs')
```

```{r}
#| label: descriptive-cod-snd-stats
cod_n_series <- cod_rounds |> 
  distinct(series_id) |> 
  nrow()

cod_n_rounds <- cod_rounds |> 
  distinct(series_id, round) |> 
  nrow()

cod_o_win_prop <- cod_rounds |> 
  filter(is_offense) |> 
  count(win_round) |> 
  mutate(prop = n / sum(n)) |> 
  filter(win_round) |> 
  pull(prop)

cod_o_win_prop_by_game <- cod_rounds |> 
  filter(is_offense) |> 
  count(game, win_round) |> 
  group_by(game) |> 
  mutate(prop = n / sum(n)) |> 
  ungroup() |> 
  filter(win_round) |> 
  select(game, n, prop)

pull_cod_o_win_prop_by_game <- function(.game) {
  cod_o_win_prop_by_game |> 
    filter(game == .game) |> 
    pull(prop) |> 
    scales::percent(accuracy = 0.1)
}
```

# Introduction

## Description of Call of Duty Search and Destroy

Call of Duty (CoD), first released in 2003, is one of the most popular first-person shooter (FPS) video game franchises of all-time. The most popular mode in the competitive scene is "Search and Destroy" (SnD).[^1] SnD is a one-sided game mode in which one team, the offensive side, tries to destroy one of two designated bomb sites on the map.

[^1]: SnD bears resemblance to "Bomb Defusal" in Counter-Strike and "Plant/Defuse" in Valorant, two other FPS games played in more popular professional leagues.

In professional CoD SnD, a team take turns playing offense and defense every round. They must win six rounds to win the series.[^2] A round can end in one of five ways:

[^2]: A maximum of 11 even rounds can be played. There is no "sudden death" or "win by two" rule like there are for SnD equivalent in professional Counter-Strike and Valorant matches.

1.  One team eliminates all members of the other team prior to a bomb plant. (Eliminating team wins.)
2.  The offensive team eliminates all members of the defensive team after a bomb plant.[^3] (Offense wins.)
3.  The defensive team defuses the bomb after a bomb plant.[^4] (Defense wins.)
4.  The offensive team does not make a plant by the time the round timer ends. (Defense wins.)

[^3]:
    -   The bomb can be picked up by any member of the offensive team.
    -   The bomb carrier is not obstructed at all by carrying the bomb (i.e. movement is the same, weapon usage is the same).
    -   The defense does not get any visual indication for who is carrying the bomb.
    -   A bomb plant takes five seconds. The timer resets if the player stops planting site prior to completing it.
    -   A bomb defuse takes seven seconds. The timer resets if the player "drops" the bomb.
    -   The bomb takes 45 seconds to defuse after being planted.

[^4]: Often the defensive team will try to eliminate all team members prior to making the defuse, but in some cases, they may try to "ninja" defuse.

I adopt the terminology "series" to refer to what CoD SnD players typically call a "match", so as to emulate the terminology of playoff series in professional leagues like the National Basketball Association, National Hockey League, and Major League Baseball. A "game" or a "match" in such leagues is analogous to a "round" of CoD SnD.

## Data

CoD has roughly gone through three eras of professional gaming: (1) Major League Gaming (MLG) tournaments prior to 2016; (2) the CoD World League (CWL), initiated in 2016; and (3) the 12-franchise CoD League (CDL), operating since 2020. The CDL has completed three year-long "seasons" as of August 2022.[^5]

[^5]: CoD is fairly unique compared to other esports in that it runs on an annual lifecycle (released coming in the late fall), where a new game is published every year under the same title. Each new game bears resemblance to past ones, often introducing relatively small variations ("improvements") to graphics, game modes, and other facets of gameplay. During the CDL era, the games released have been Modern Warfare (2020), Cold War (2021) and Vanguard (2022).

The data set consists of all SnD matches played in tournanaments and qualifiers during the CDL era, totaling 7,792 rounds across 852 series. Data was collected in spreadsheets by community member "IOUTurtle".[^6]

[^6]: Data: <https://linktr.ee/CDLArchive>. Author: <https://twitter.com/IOUTurtle>

The empirical offensive round win percentage across all rounds is 47.8%.[^7] Table \ref{tbl:cod-o-win-prop-by-series-state} shows round win percentages by series "state" (i.e. the number of round wins by each team prior to an upcoming round). Offensive round win rate is not quite constant, although never veers more than 10% from this global average.

[^7]: Offensive round win percentage has been nearly constant across the three games during the CDL era: 1. 47.2% in MW (2020) 2. 47.9% in Cold War (2021) 3. 48.1% in Vanguard (2022)

```{r}
#| label: cod_o_win_prop
cod_round_and_series_win_prop_by_side <- cod_rounds |>
  group_by(pre_cumu_w, pre_cumu_l, is_offense) |>
  summarize(
    n = n(),
    across(c(win_round, win_series), sum)
  ) |>
  ungroup() |>
  mutate(
    win_round_prop = win_round / n,
    win_series_prop = win_series / n
  )

cod_round_and_series_win_prop_by_side |>
  filter(is_offense) |>
  transmute(
    pre_cumu_w,
    pre_cumu_l,
    lab = sprintf('%.1f%%\n(%s)', round(100 * win_round_prop, 1), n)
  ) |>
  pivot_wider(
    names_from = pre_cumu_l,
    values_from = lab
  ) |>
  arrange(pre_cumu_w) |>
  gt::gt() |>
  gt::cols_label(
    pre_cumu_w = "Defense round wins",
  ) |>
  gt::tab_spanner(
    label = "Offense round wins",
    columns = `0`:`5`
  ) |>
  gt::as_latex()
```

```{=tex}
\begin{longtable}{crrrrrr}
\caption{Offensive round win rates for the upcoming round, given both the offensive and defensive team's prior number of round wins}\label{tbl:cod-o-win-prop-by-series-state} \\
\toprule
& \multicolumn{6}{c}{Offense round wins} \\ 
\cmidrule(lr){2-7}
Defense round wins & 0 & 1 & 2 & 3 & 4 & 5 \\ 
\midrule
0 & 47.8\%
(852) & 46.6\%
(408) & 43.1\%
(216) & 43.5\%
(115) & 43.3\%
(67) & 40.5\%
(37) \\ 
1 & 48.6\%
(444) & 49.3\%
(418) & 51.5\%
(309) & 43.4\%
(205) & 43.3\%
(120) & 39.4\%
(99) \\ 
2 & 52.8\%
(218) & 48.9\%
(305) & 48.9\%
(315) & 46.6\%
(262) & 48.7\%
(189) & 42.1\%
(133) \\ 
3 & 54.5\%
(123) & 46.0\%
(200) & 49.6\%
(250) & 45.6\%
(248) & 44.4\%
(214) & 44.8\%
(174) \\ 
4 & 56.9\%
(65) & 54.5\%
(145) & 47.2\%
(193) & 44.7\%
(228) & 55.2\%
(221) & 50.5\%
(208) \\ 
5 & 47.4\%
(38) & 49.4\%
(83) & 47.1\%
(136) & 50.9\%
(175) & 45.2\%
(177) & 46.0\%
(202) \\ 
\bottomrule
\end{longtable}
```
# Literature review

There have been a handful of studies of the distribution of games played in a series of a professional sport. Most assume a constant probability $\phi$ of a given team winning a game in the series, regardless of the series state. Mosteller [-@mosteller1952] observed that the American League had dominated the National League in Major League Baseball's (MLB) World Series matchups, implying that matchups should not modeled with $\phi = 0.5$. Mosteller proposed three approaches for identifying the optimal constant probability value of the stronger team in the World Series, finding $\phi \approx 0.65$. in each case: (1) solving for $p$ from the empirical average number of games won by the loser of the series, which he called the "method of moments" approach; (2) maximizing the likelihood that the sample would have been drawn from a population in which the probability of a team winning a game is constant across the series (i.e. maximum likelihood), and (3) minimizing the chi-square goodness of fit statistic for $\phi$.

Chance [-@chance2020] re-examines the constant probability notion in Major League Baseball's (MLB) World Series (1923--2018), the National Basketball Association's (NBA) Finals (1951--2018), and the National Hockey League's (NHL) Stanley Cup (1939--2018). Chance finds strong evidence against the null hypothesis of $\phi = 0.5$ in the MLB and NHL championship series when applying Mosteller's first and second methods. Chance goes on to outline a conditional probability framework (likelihood of winning a game given the series state) which can exactly explain the distribution of the number of games played.

Momentum, one of most discussed topics in sports analytics, goes hand-in-hand with a discussion of the nature of series outcomes.[^8] Two opposing fallacies are observed in the context of momentum: the "gambler's fallacy" (negative recency) and "hot hand fallacy" (positive recency). Per Ayton et al. [-@ayton2004], negative recency is "the belief that, for random events, runs of a particular outcome ... will be balanced by a tendency for the opposite outcome", while positive recency is the expectation of observing future results that match recent results.

[^8]: We often use use "streaks" and momentum interchangeably, but as [@steeger2021] note, momentum implies dependence between events, whereas streaking does not.

Studying both player streaks and team streaks in basketball, in both observational and controlled settings. Gilovich et al. [-@gilovich1985], henceforth GVT, do not find evidence for the hot hand phenomenon. However, Miller and Sanjurjo [-@miller2018], henceforth MS, refuted the conclusions of GVT, proving a framework for quantifying streak selection bias, which effectively works in the manner posited by the gambler's fallacy[^9]. Specifically, MS say that a "bias exists in a common measure of the conditional dependence of present outcomes on streaks of past outcomes in sequential data" implying that, under i.i.d. conditions, "the proportion of successes among the outcomes that immediately follow a streak of consecutive successes is expected to be strictly less than the underlying (conditional) probability of success". When applying streak selection bias to GVT.'s data, MS come to the opposite conclusions as GVT.

[^9]: Streak selection bias, "in conjunction with a quasi-Bayesian model of decision making under sample size neglect... provides a novel structural candidate explanation for the persistence of gambler's fallacy beliefs", per MS.

Despite the plethora of existing research on games played in a series and momentum in sports, these topics have yet to be investigated heavily in esports. Work has been done to examine in-round win probability in other FPS titles such as Counter-Strike [@xenopoulos2022] and Valorant [@derover2021], both of which are round-based like CoD SnD. However, research on round-level trends is sparse, perhaps because games like Counter-Strike and Valorant both have economic aspects that can create clear advantages on side in a given round, given how prior rounds played out.

Additionally, both Counter-Strike and Valorant have overtime rules and blocked offensive/defensive roles (i.e. playing either offense or defense for many consecutive rounds). On the other hand, teams in CoD SnD rotate sides every round, analogous to a 1-1-1-1-1-1-1 format for home advantage in best-of-seven series for professional sports like the MLB, NBA, and NHL.[^10] While theoretically one might be able to account for any kind of balanced rotation, such as a "blocky" on like a 5-5-1, the rotation of team sides every round is convenient, particularly for convincing ourselves that rounds could reasonably be modeled as i.i.d. Bernoulli trials.

[^10]: 1-1-1-1-1-1-1 is not used today in these leagues, but it was at least one in each league.

# Methodology

## Distribution of rounds played {#sec:method-rounds-played}

The formula for the expected proportion of series a best-of-$s$ format ending in $r$ rounds ($r \leq s$), $\hat{\Phi}(r)$, for a constant round win probability $\phi$ ($0 \leq \phi \leq 1$) for one team[^11] is

[^11]: If $\phi > 0.5$, then one might say that this team is the better team (known in hindsight).

```{=tex}
\begin{equation}\protect\hypertarget{eq:m}{}{
m = \frac{s + 1}{2}
}\label{eq:m}\end{equation}
```
```{=tex}
\begin{equation}\protect\hypertarget{eq:series-length}{}{
\hat{\Phi}(r) = \frac{(r - 1)!}{(m - 1)!(r - s)!}(\phi^{m}(1 - \phi)^{r - m} + \phi^{r - m}(1 - \phi)^m)
}\label{eq:series-length}\end{equation}
```
$s = 11$ rounds for CoD SnD.[^12]

[^12]: $\phi$ and other symbols are selected so as to avoid reserve symbols, like $p$ for usage in other contexts, without risking confusion. Upper-case symbol, e.g. $\Phi$, are consistently used in this paper to represent proportions, while lower-case symbols corresponding to proportions represent probabilities. "Hats", e.g. $\hat{\Phi}$, are used to convey expectations, while "bare" symbols mirroring hat-ed symbols convey observational data.

To evaluate the constant round win probability null hypothesis---that is, that the expected and observed round win rates, $\hat{\Phi}(r)$ and $\Phi(r)$ respectively, are equal to one another---we can compute the chi-square goodness of fit statistic, $\chi^2$,

```{=tex}
\begin{equation}\protect\hypertarget{eq:chi-squ}{}{
\chi^2 = \sum^R \frac{(\Phi(r) - \hat{\Phi}(r))^2}{\hat{\Phi}(r)}.
}\label{eq:chi-squ}\end{equation}
```
$r \in R = [6, 7, 8, 9, 10, 11]$ for CoD SnD.

## Momentum {#sec:method-momentum}

Let us now consider round win rate immediately following a streak of $k$ wins, given that the series lasts $r$ rounds. Removing our knowledge of a streak, we might model the Bernoulli round win probability as

```{=tex}
\begin{equation}\protect\hypertarget{eq:pwr}{}{
p_0(\text{win} | r) = p^{w|r}_0 = \{
\begin{array}{rcll}
p^{w|r_W} & = & \frac{m}{r}, & \text{team wins series} \\
p^{w|r_L} & = & \frac{r - m}{r}, & \text{team loses series}
\end{array}
}
\label{eq:pwr}
\end{equation}
```
using $m$ from Equation \ref{eq:m}. The Bernoulli round loss probability for a series lasting $r$ rounds, $p^{\ell|kr}_0$ (i.e. $\ell = 1 - w, w \in [0, 1]$), can be formulated symmetrically.

As shown by MS, we should expect the proportion of rounds wins immediately following a streak of $k$ rounds wins for a series lasting $r$ rounds, $\hat{P}^{w|kr}_{MS}$, to be strictly less than $p^{w|r}$.[^13][^14]

[^13]: See Appendix E.1 of MS for details on the proof that follows.

[^14]: Here, I choose symbols, e.g. $\pi$, $x$, etc. that mostly do not conflict with syntax in other places. The exceptions are $k$ and $r$, which are used to represent streak length and number of trials globally.

> Theorem 1: *Let* $\mathbf{X} = \{X_i\}^{r}_{i=1}, r \geq 3$*, be a sequence of independent Bernoulli trials, each with probability of success* $0 < \pi < 1$*. Let* $\hat{\Pi}_k(\mathbf{X})$ *be the proportion of successes on the subset of trials* $I_k(\mathbf{X})$ *that immediately follow* $k$ consecutive successes, that is, $\hat{\Pi}_k(\mathbf{X}) := \sum_{i \in I_k(\mathbf{X})} X_i | I_k(\mathbf{X}) |. \hat{\Pi}_k$ *is a biased estimate of* $\mathbb{P}(X_t = 1 | \prod_{j=t-k}^{t-1} X_j = 1) \equiv \pi$ *for all* $k$ *such that* $1 \leq k \leq r - 2$*. In particular,* $E[\hat{\Pi}_k(\mathbf{X}) | I_k(\mathbf{X}) \neq \emptyset] < \pi.$

MS notes that there does not exists a closed form representation for $E[\hat{\Pi}_k(\mathbf{X}) | I_k(\mathbf{X}) \neq \emptyset]$, henceforth $E^\Pi$, for $k \> 1$. One may recursively build a collection of dictionaries to quantify $E^\Pi$ analytically, or, more reasonably for large values of $n$, one may run simulations to estimate the expected value. Indeed, Figure 1 uses simulation to estimate $E^\Pi$ as a function of the total number of trials $r$, given $\pi$ and $k$.[^15][^16]

[^15]: I've adapted the code from Vafa [-@vafa2017], which implements MS's framework [-@miller2018].

[^16]: Note that $\pi$ and $\hat{\Pi}_k(\mathbf{X})$ are analogues for $p_0^{w|r}$ and $\hat{P}^{w|kr}_{MS}$ in my notation for CoD SnD.

```{r}
#| label: npkd
## http://keyonvafa.com/hot-hand/
get_post_streak_prob <- function(n, k, p = 0.5) {
  tosses <- rbinom(n, 1, p)
  runs <- rle(tosses)
  n_neg_after <- length(which(runs$values == 1 & runs$lengths >= k))
  n_pos_after <- sum(runs$lengths[which(runs$values == 1 & runs$lengths >= k)] - k)
  
  ## edge case
  if (n %in% cumsum(runs$lengths)[which(runs$values == 1 & runs$lengths >= k)]) {
    n_neg_after <- n_neg_after - 1
  }
  
  n_pos_after / (n_pos_after + n_neg_after)
}

simulate_post_streak_prob <- function(sims = 10000, seed = 42, ...) {
  withr::local_seed(seed)
  rerun(
    sims,
    get_post_streak_prob(...)
  ) |> 
    flatten_dbl() |> 
    mean(na.rm = TRUE)
}

# # set.seed(42)
# # runs <- rerun(
# #   1000,
# #   crossing(
# #     n = 1:50,
# #     k = 2:4,
# #     p = 0.5
# #   ) |>
# #     mutate(
# #       next_p = pmap_dbl(list(n, k, p), ~simulate_post_streak_prob(sims = 1000, n = ..1, k = ..2, p = ..3))
# #     )
# # ) |>
# #   reduce(bind_rows)
# # qs::qsave(runs, 'data/runs_1000x1000_nkp.qs')
# 
# library(ggplot2)
# library(latex2exp)
# runs <- qs::qread('data/runs_1000x1000_nkp.qs')
# 
# agg_runs <- runs |>
#   filter(!is.nan(next_p)) |>
#   group_by(p, k, n) |>
#   summarize(
#     across(next_p, mean)
#   ) |>
#   ungroup() |>
#   arrange(p, k)
# 
# # theme_set(theme_classic())
# 
# p <- agg_runs |>
#   arrange(p, k, n) |>
#   mutate(
#     across(k, factor)
#   ) |>
#   ggplot() +
#   aes(x = n, y = next_p, color = k, group = k) +
#   guides(
#     color = guide_legend(
#       title = 'k',
#       title.hjust = 0.5,
#       nrow = 1, override.aes = list(size = 3)
#     )
#   ) +
#   geom_line() +
#   geom_hline(aes(yintercept = p)) +
#   labs(x = 'r', y = expression(pi)) +
#   theme_classic() +
#   theme(
#     legend.position = c(0.5, 0.8)
#   )
# p
# 
# ggsave(
#   p,
#   filename = 'paper/images/pwkr_ms.png',
#   width = 5.5,
#   height = 3
# )
```

```{=tex}
\begin{figure}
\centering
\includegraphics{images/pwkr_ms.png}
\caption{The expected value of the proportion of successes on trials, $E^\Pi$, that immediately follow $k$ consecutive successes for Bernoulli trial success $\pi = 0.5$ and streak lengths $k \in [2, 3, 4]$, as a function of the total number of trials $r$.}
\end{figure}
```
My context is fundamentally different from that of GVT and MS, both of whom focus on longitudinal data in controlled settings. GVT also performs statistical tests on shots from players in live games, i.e. "observational" data, but they note that their findings are likely affected player shot selection in the face of defensive strategy by the opposing team. The number of trials is fixed in their experimental designs, but in CoD SnD, the number of rounds played is determined as a function of the max number of possible rounds ($s$) and whether or not the team wins the series, as shown in Equation \ref{eq:pwr}. The Bernoulli trial success probability, i.e. a round win in CoD SnD, is not independent of the opponent. Consequently, a statistical test of the difference in $\hat{P}^{w|kr}$ and $\hat{P}^{\ell|kr}$, as performed by GVT and MS to evaluate their respective hypothesis regarding post-streak success rates is not appropriate.

One might consider another form of the expected proportion of rounds won immediately after a streak of $k$ round wins in a best-of-$s$ series given the length of the series ($r$ rounds), the "notional" proportion $\hat{P}^{w|kr}_0$. The Bernoulli round win probability underlying $\hat{P}^{w|kr}_0$ is

```{=tex}
\begin{equation}\protect\hypertarget{eq:pwkr}{}{
p_0(\text{win} | k, r) = p^{w|kr}_0 =\{
\begin{array}{rcll}
p_0^{w|kr_W} & = & \frac{m - k}{i - k}, & \text{team wins series} \\
p_0^{w|kr_L} & = & \frac{s - m - k}{i - k}, & \text{team loses series}
\end{array}
}
\label{eq:pwkr}
\end{equation}
```
using $m$ from Equation \ref{eq:m}. We can perform a binomial test to test the null hypothesis $H_0 : \omega = \omega_0$ for the observed probability of success $\omega$ and a user-specified $\omega_0$, where $0 \leq \omega_0 \leq 1$.[^17]

[^17]: $\omega$ and other symbols are chosen so as to not conflict with symbols already used elsewhere for in other equations, either generic or for CoD SnD specifically.

If there are $\sigma$ observed successes in a sample of $r$ trials and we expect that there should be $r * \omega_0$, the probability of getting this expected number of successes is

```{=tex}
\begin{equation}\protect\hypertarget{eq:binom}{}{
\Pr(\sigma) = {\binom {r}{\sigma}} \omega^{\sigma}(1-\omega)^{r-\sigma}.
}\label{eq:binom}\end{equation}
```
Plugging in the notional probability $p^{w|kr}_0$ for the null $\omega_0$ and the observed probability $p^{w|kr} = N^{w|kr} \times P^{w|kr}$, where $N^{w|kr}$ is the observed number of instances where a teams wins the round immediately following a $k$-round winning streak in a series lasting $r$ rounds, for the observed $\omega$, we can evaluate whether the observed probability is not significantly different from the notional probability. If we reject this hypothesis, then we might consider team momentum, represented by post-streak success, plausible.

Returning to the streak-selection adjustment framework proposed by MS, one can perform the same binomial test, swapping out $p^{w|kr}_0$ with $p^{w|kr}_{MS}$. Of course, given the caveats mentioned before, the results should be heeded with caution.

# Results

First, I investigate the constant probability assumption and the distribution of rounds played in a series. Chance's [-@chance2020] work is closely related to mine, and, in fact, provides a guide for this investigation. Afterwards, I investigate post-streak win rates and the context of momentum, leveraging MS's [-@miller2018] framework for streak selection bias.

## Distribution of rounds played {#sec:results-rounds-played}

Using Equation \ref{eq:chi-squ}, I find that $\chi^2 = 16.0$ (p-value of 0.0068) for the naive constant round win probability $\phi_0 = 0.5$. Thus, I can comfortably reject the constant probability null hypothesis for $\phi_0 = 0.5$, even at a confidence level of $\alpha = 0.01$.

```{r}
#| label: cod_series_outcome_prop
.max_round <- 11
.cutoff <- 6
prob_of_series_lasting_r_rounds <- function(r, p = 0.5, s) {
  m <- as.integer((s + 1) / 2)
  (factorial(r - 1) / (factorial(m - 1) * factorial(r - m))) * (p^m * (1 - p)^(r - m) + p^(r - m) * (1 - p)^m)
}

expected_series_streaks_of_outcomes <- function(m, n) {
  factorial(m + n) / (factorial(m) * factorial(n))
}

# https://raw.githubusercontent.com/dgrtwo/splittestr/master/R/vectorized-prop-test.R
vectorized_prop_test_approx <- function(a, b, c, d) {
  n1 <- a + b
  n2 <- c + d
  n <- n1 + n2
  p <- (a + c) / n
  E <- cbind(p * n1, (1 - p) * n1, p * n2, (1 - p) * n2)
  
  x <- cbind(a, b, c, d)
  
  DELTA <- a / n1 - c / n2
  YATES <- pmin(.5, abs(DELTA) / sum(1 / n1 + 1 / n2))
  
  STATISTIC <- rowSums((abs(x - E) - YATES)^2 / E)
  PVAL <- pchisq(STATISTIC, 1, lower.tail = FALSE)
  PVAL
}

vectorized_prop_test_exact <- function(a, b, c, d) {
  sapply(seq_along(a), function(i) {
    fisher.test(cbind(c(a[i], c[i]), c(b[i], d[i])))$p.value
  })
}

vectorized_prop_test <- function(x1, n1, x2, n2, conf.level = 0.95) {
  a <- x1
  b <- n1 - x1
  c <- x2
  d <- n2 - x2
  
  # if any values are < 20, use Fisher's exact test
  exact <- (a < 20 | b < 20 | c < 20 | d < 20)
  
  pvalue <- rep(NA, length(a))
  
  if (any(exact)) {
    pvalue[exact] <- vectorized_prop_test_exact(a[exact], b[exact], c[exact], d[exact])
  }
  if (any(!exact)) {
    pvalue[!exact] <- vectorized_prop_test_approx(a[!exact], b[!exact], c[!exact], d[!exact])
  }
  
  mu1 <- a / (a + b)
  mu2 <- c / (c + d)
  
  ## confidence interval
  alpha2 <- (1 - conf.level) / 2
  DELTA <- mu2 - mu1
  WIDTH <- qnorm(alpha2)
  alpha <- (a + .5) / (a + b + 1)
  beta <- (c + .5) / (c + d + 1)
  
  n <- n1 + n2
  YATES <- pmin(.5, abs(DELTA) / sum(1 / n1 + 1 / n2))
  
  z <- qnorm((1 + conf.level) / 2)
  WIDTH <- z * sqrt(mu1 * (1 - mu1) / n1 + mu2 * (1 - mu2) / n2)
  
  tibble(
    estimate = DELTA,
    conf.low = pmax(DELTA - WIDTH, -1),
    conf.high = pmin(DELTA + WIDTH, 1),
    p.value = pvalue
  )
}

cod_actual_round_streaks <- cod_rounds |> 
  filter(round <= .max_round) |> 
  filter(win_series) |> 
  mutate(across(win_round, as.integer)) |> 
  group_by(series_id) |> 
  summarize(
    wins = max(cumu_w),
    losses = max(cumu_l),
    ws = paste0(win_round, collapse = '-')
  ) |> 
  ungroup() |> 
  mutate(n_rounds = wins + losses) |> 
  unite(
    record, wins, losses, sep = '-'
  ) |>
  count(record, n_rounds, ws, sort = TRUE) |> 
  mutate(prop = n / sum(n))

summarize_cod_streaks <- function(p = 0.5) {
  
  expected_round_streaks <- tibble(
    n_rounds = .cutoff:.max_round
  ) |> 
    mutate(
      series_prop = map_dbl(n_rounds, ~prob_of_series_lasting_r_rounds(.x, s = .max_round, p = !!p)),
      n_expected_series_streaks = map_dbl(n_rounds, ~expected_series_streaks_of_outcomes(.cutoff, .x - .cutoff))
    ) |> 
    transmute(
      n_rounds,
      series_prop,
      prop = series_prop / n_expected_series_streaks
    )
  
  round_streaks <- full_join(
    cod_actual_round_streaks |> 
      rename_with(~sprintf('%s_actual', .x), c(n, prop)),
    expected_round_streaks |> 
      rename_with(~sprintf('%s_expected', .x), prop),
    by = 'n_rounds'
  )
  
  round_streak_prop <- round_streaks |> 
    drop_na() |> 
    mutate(
      prop_diff = prop_actual - prop_expected,
      total_actual = sum(n_actual),
      n_expected = round(prop_expected * total_actual),
      p = vectorized_prop_test(n_actual, total_actual, n_expected, total_actual)
    ) |> 
    select(-total_actual) |> 
    unnest_wider(p) |> 
    arrange(p.value)
  
  series_outcomes <- full_join(
    cod_actual_round_streaks |> 
      group_by(record, n_rounds) |> 
      summarize(
        across(n, sum)
      ) |> 
      ungroup() |> 
      mutate(prop = n / sum(n)) |> 
      rename_with(~sprintf('%s_actual', .x), c(n, prop)),
    expected_round_streaks |> 
      select(n_rounds, prop_expected = series_prop),
    by = 'n_rounds'
  )
  
  series_outcome_prop <- series_outcomes |> 
    drop_na() |> 
    mutate(
      prop_diff = prop_actual - prop_expected,
      total_actual = sum(n_actual),
      n_expected = round(prop_expected * total_actual),
      p = vectorized_prop_test(n_actual, total_actual, n_expected, total_actual)
    ) |> 
    select(-total_actual) |> 
    unnest_wider(p) |> 
    arrange(p.value)
  
  list(
    rounds = round_streak_prop,
    series = series_outcome_prop
  )
}

cod_streaks_naive_res <- summarize_cod_streaks(p = 0.5)
```

Table \ref{tbl:cod-prob-series-lasting-r-rounds} shows the expected series lasting $r$ rounds, $\hat{\Phi}_0(r)$ (under the assumption $\phi_0 = 0.5$), for $s = 11$, along with the observed proportions, $\Phi(r)$, in CoD SnD.

```{=tex}
\begin{longtable}{rrrr}
\caption{The expected proportion of series lasting $r$ rounds ($\hat{\Phi}_0(r)$) in a best-of-11 format, wehre $r \in R = [6, 7, 8, 9, 10, 11]$ under the assumption that each team has a constant round win probability $\phi_0 = 0.5$. The observed frequencies for CoD SnD are shown as a count $N(r)$ and as a proportion $\Phi(r)$ of all series ($\sum^R N(r)$).}\label{tbl:cod-prob-series-lasting-r-rounds} \\
\toprule
$r$ & $\hat{\Phi}_0(r)$ & $\Phi(r)$ & $N(r)$ \\ 
\midrule
6 & $3.1\%$ & $4.7\%$ & 40 \\ 
7 & $9.4\%$ & $11.9\%$ & 101 \\ 
8 & $16.4\%$ & $16.5\%$ & 141 \\ 
9 & $21.9\%$ & $21.7\%$ & 185 \\ 
10 & $24.6\%$ & $21.5\%$ & 183 \\ 
11 & $24.6\%$ & $23.7\%$ & 202 \\ 
\bottomrule
\end{longtable}
```
```{r}
#| label: cod_series_outcomes_naive_chi
generate_chi_label <- function(statistic, p.value) {
  sprintf('%.1f (%s)', statistic, ifelse(p.value <= 0.01, '<=0.01', as.character(round(p.value, 2))))
}

perform_series_chi_test <- function(series) {
  chisq.test(
    series$n_actual, 
    p = series$prop_expected
  ) |> 
    broom::tidy()
}

cod_series_outcomes_naive_chi <- cod_streaks_naive_res$series |> 
  perform_series_chi_test()
```

```{r}
#| label: cod-alternative-to-p=0.5-method-prep
min_p1 <- 0.5
max_p1 <- 0.7
interval_p1 <- 0.0025
cod_ps <- tibble(p = seq(min_p1, max_p1, by = 0.0025))
```

```{r}
#| label: cod-alternative-to-p=0.5-method-1
## p. 365 on https://math.mit.edu/classes/18.095/2016IAP/lec9/Sports_Mosteller1952_WorldSeries.pdf
cod_prob_of_series_lasting_r_rounds <- function(r, p) {
  prob_of_series_lasting_r_rounds(r = r, p = p, s = 11)
}

theoretical_cod_series_length <- function(p) {
  6 * cod_prob_of_series_lasting_r_rounds(r = 6, p = p) +
    7 * cod_prob_of_series_lasting_r_rounds(r = 7, p = p) +
    8 * cod_prob_of_series_lasting_r_rounds(r = 8, p = p) +
    9 * cod_prob_of_series_lasting_r_rounds(r = 9, p = p) +
    10 * cod_prob_of_series_lasting_r_rounds(r = 10, p = p) +
    11 * cod_prob_of_series_lasting_r_rounds(r = 11, p = p)
}

cod_rounds_per_series <- cod_streaks_naive_res$series |>
  summarize(
    actual = sum(n_rounds * prop_actual),
    expected = sum(n_rounds * prop_expected)
  )

cod_theoretical_series_lengths1 <- cod_ps |>
  mutate(
    theoretical_series_length = map_dbl(p, theoretical_cod_series_length),
    diff = theoretical_series_length - cod_rounds_per_series$actual
  )

cod_theoretical_p1 <- cod_theoretical_series_lengths1 |>
  slice_min(abs(diff), n = 1) |>
  pull(p)
```

```{r}
#| label: cod-alternative-to-p=0.5-method-2
pluck_cod_n <- function(.n_rounds) {
  cod_streaks_naive_res$series |>
    filter(n_rounds == .n_rounds) |>
    pull(n_actual)
}

cod_n6 <- pluck_cod_n(6)
cod_n7 <- pluck_cod_n(7)
cod_n8 <- pluck_cod_n(8)
cod_n9 <- pluck_cod_n(9)
cod_n10 <- pluck_cod_n(10)
cod_n11 <- pluck_cod_n(11)

## This adjustment is something that is not done in the paper. I do it here to reduce
##   the magnitude of the exponents.
cod_pm <- pmin(cod_n6, cod_n7, cod_n8, cod_n9, cod_n10, cod_n11)
cod_n6adj <- round(10 * cod_n6 / cod_pm)
cod_n7adj <- round(10 * cod_n7 / cod_pm)
cod_n8adj <- round(10 * cod_n8 / cod_pm)
cod_n9adj <- round(10 * cod_n9 / cod_pm)
cod_n10adj <- round(10 * cod_n10 / cod_pm)
cod_n11adj <- round(10 * cod_n11 / cod_pm)

maximize_cod_series_p <- function(p) {
  cod_prob_of_series_lasting_r_rounds(r = 6, p = p)^cod_n6adj *
    cod_prob_of_series_lasting_r_rounds(r = 7, p = p)^cod_n7adj *
    cod_prob_of_series_lasting_r_rounds(r = 8, p = p)^cod_n8adj *
    cod_prob_of_series_lasting_r_rounds(r = 9, p = p)^cod_n9adj *
    cod_prob_of_series_lasting_r_rounds(r = 10, p = p)^cod_n10adj *
    cod_prob_of_series_lasting_r_rounds(r = 11, p = p)^cod_n11adj
}

cod_theoretical_series_lengths2 <- cod_ps |>
  mutate(
    theoretical_series_length = map_dbl(p, maximize_cod_series_p)
  )

cod_theoretical_p2 <- cod_theoretical_series_lengths2 |>
  slice_max(theoretical_series_length, n = 1) |>
  pull(p)
```

```{r}
#| label: cod-alternative-to-p=0.5-method-3
summarize_cod_series_chi <- function(p) {
  summarize_cod_streaks(p) |> 
    pluck('series') |> 
    perform_series_chi_test()
}

cod_theoretical_series_lengths3 <- cod_ps |> 
  filter(p >= 0.56, p < 0.585) |> 
  mutate(
    chi_squ = map_dbl(
      p,
      ~summarize_cod_series_chi(.x) |> 
        pluck('statistic')
    )
  )

cod_theoretical_p3 <- cod_theoretical_series_lengths3 |> 
  slice_min(chi_squ, n = 1) |> 
  pull(p)
```

```{r}
#| label: cod-alternatives-to-p=0.5
cod_streaks_other_res <- tibble(
  p = c(0.5, cod_theoretical_p1, cod_theoretical_p2, cod_theoretical_p3),
  idx = 0L:3L,
  name = c('0. Naive', '1. Method of moments', '2. Maximum likelihood', '3. Minimum Chi-square')
) |> 
  mutate(
    series_streaks = map(
      p,
      ~summarize_cod_streaks(.x) |> 
        pluck('series') 
    ),
    chi_squ = map(
      series_streaks,
      ~.x |> 
        perform_series_chi_test() |> 
        select(statistic, p.value)
    )
  )
```

Table \ref{tbl:mosteller-methods-results} shows the alternate values for the constant round win probability that I find when applying the three methods suggested by Mosteller [-@mosteller1952]. Each is approximately or equal to 0.575, and, when applying Equation \ref{eq:chi-squ}, each results in a $\chi^2$ value for which I cannot reject the constant probability null hypothesis.

```{=tex}
\begin{longtable}[]{@{}lrr@{}}
\caption{Alternate estimates of the constant probability ($\phi$) for winning a given round in a CoD SnD, applying the three methods suggested by Mosteller (1952), in addition to the naive ($\phi_0 = 0.5$).}\label{tbl:mosteller-methods-results} \\
\toprule()
Method & $\phi$ & $\chi^2$ (p-value) \\
\midrule()
\endfirsthead
\toprule()
Method & $\phi(r)$ & $\chi^2$ (p-value) \\
\midrule()
\endhead
0. Naive & 0.5000 & 16.0 (\textless=0.01) \\
1. Method of moments & 0.5725 & 3.6 (0.6) \\
2. Maximum likelihood & 0.5750 & 3.5 (0.62) \\
3. Minimum ($\chi^2$) & 0.5775 & 3.5 (0.62) \\
\bottomrule()
\end{longtable}
```
Table \ref{tbl:expected-series-lengths-alternative-ps} shows the new $\hat{\Phi}(r)$ when re-applying Equation \ref{eq:series-length} for the maximum likelihood estimate $\phi_2 = 0.575$, resulting in a new set of expected proportions of series lasting $r$ rounds $\hat{\Phi}_2(r)$.[^18] I observe that $\hat{\Phi}_2(r)$ is larger than $\hat{\Phi}_0(r)$ for $r \in [6, 7]$, more closely matching $\Phi(r)$. $\hat{\Phi}_2(r)$ is also closer to the observed $\Phi(r)$ for $r \in [9, 10]$, although not for $r \in [8, 11]$.

[^18]: The method of moments and minimum $\chi^2$ estimates for $\phi$ are omitted simply because the results would be nearly identical to those for the maximum likelihood estimate of $\phi$ (since they are all $\approx 0.575$).

```{=tex}
\begin{longtable}[]{@{}rrrr@{}}
\caption{The observed proportion of series $\Phi(r)$ ending in $r$ rounds in CoD SnD's best-of-11 format, compared to the expected proportion $\hat{\Phi}_0(r)$ the naive assumption $\phi_0 = 0.5$ and the expected proportion $\hat{\Phi}_2(r)$ under the maximum likelihood estimate $\phi_2 = 0.575$ for constant round win probability.}\label{tbl:expected-series-lengths-alternative-ps} \\
\toprule()
$r$ & $\hat{\Phi}_0(r)$ = 0.5 & $\hat{\Phi}_2(r)$ = 0.575 & $\Phi(r)$ \\
\midrule()
\endhead
6 & 3.1\% & 4.2\% & 4.7\% \\
7 & 9.4\% & 11.2\% & 11.9\% \\
8 & 16.4\% & 17.8\% & 16.5\% \\
9 & 21.9\% & 21.8\% & 21.7\% \\
10 & 24.6\% & 23.0\% & 21.5\% \\
11 & 24.6\% & 22.0\% & 23.7\% \\
\bottomrule()
\end{longtable}
```
Observing that $\hat{\Phi}_2(r)$ reasonably matches $\Phi(r)$ (especially in comparison to $\hat{\Phi}_0(r)$), along with the null hypothesis rejection shown in Table \ref{tbl:mosteller-methods-results}, it is fair to conclude that the constant round win probability assumption can be valid in CoD SnD series with the appropriate choice of $\phi$ ($\approx 0.575$).

## Momentum {#sec:results-momentum}

```{r}
#| label: cod_round_win_prop_after_k_wins
vectorized_binom_test <- function(x, n, p, ...) {
  map_dfr(
    seq_along(x), 
    function(i) {
      res <- binom.test(x[i], n[i], p = p[i], ...)
      tibble(
        estimate = res$estimate,
        conf.low = res$conf.int[[1]],
        conf.high = res$conf.int[[2]],
        p.value = res$p.value
      )
    }
  )
}

cod_round_streaks <- cod_rounds |> 
  group_by(series_id, team) |> 
  mutate(
    won_prior_round1 = lag(win_round, n = 1, default = NA),
    won_prior_round2 = lag(win_round, n = 2, default = NA),
    won_prior_round3 = lag(win_round, n = 3, default = NA),
    won_prior_round4 = lag(win_round, n = 4, default = NA),
    won_prior_round5 = lag(win_round, n = 5, default = NA)
  ) |> 
  ungroup() |>
  select(
    series_id,
    team,
    is_offense,
    round,
    n_rounds,
    win_series,
    win_round,
    starts_with('won_prior_round')
  )

postprocess_round_streaks <- function(k, probs_nx_k, ...) {
  
  cols <- sprintf('won_prior_round%d', 2:k)
  col_syms <- syms(cols)
  cod_round_streaks_after_x <- cod_round_streaks |> 
    drop_na(!!!col_syms) |> 
    count(n_rounds, win_series, ..., win_round, won_prior_round1, !!!col_syms, sort = TRUE)
  
  cod_round_win_prop_after_z <- cod_round_streaks_after_x |> 
    filter(won_prior_round1, !!!col_syms)
  
  suppressMessages(
    cod_round_win_prop_after_z_with_probs <- cod_round_win_prop_after_z |> 
      group_by(win_series, n_rounds, ...) |> 
      mutate(
        total = sum(n),
        prop = n / total
      ) |> 
      ungroup() |> 
      select(n_rounds, win_round, win_series, ..., n, total, prop) |> 
      # filter(!win_round) |> 
      arrange(n_rounds, win_round, ...) |> 
      inner_join(
        probs_nx_k
      )
  )
  
  cod_round_win_prop_after_z_with_probs |> 
    mutate(
      notional_p_value = vectorized_binom_test(n, total, p = notional_prop)$p.value,
      ms_p_value = vectorized_binom_test(n, total, p = ms_prop)$p.value
    )
}

summarize_cod_win_prop_after_k_wins <- function(k, ...) {
  probs_nx_k <- crossing(
    n_rounds = 7L:11L,
    win_series = c(TRUE, FALSE),
    win_round = c(TRUE, FALSE)
  ) |> 
    mutate(
      is_valid = case_when(
        win_series ~ TRUE,
        (n_rounds - !!k - 6L) >= 0L ~ TRUE,
        TRUE ~ FALSE
      )
    ) |> 
    filter(is_valid) |> 
    select(-is_valid) |> 
    mutate(
      naive_prop = ifelse(win_series, 6L / n_rounds, (n_rounds - 6L) / n_rounds),
      notional_prop = (ifelse(win_series, 6L, n_rounds - 6L) - k) / (n_rounds - k),
      across(c(naive_prop, notional_prop), ~ifelse(win_round, .x, 1 - .x)),
      ms_prop = map2_dbl(
        n_rounds, naive_prop, 
        ~simulate_post_streak_prob(n = ..1, k = !!k, p = ..2)
      ),
      across(
        ms_prop,
        ~ifelse(win_round, .x, naive_prop + (naive_prop - .x))
      ),
      across(
        ms_prop,
        ~ifelse(is.nan(ms_prop), naive_prop, .x)
      )
    )
  
  postprocess_round_streaks(
    k = k,
    probs_nx_k = probs_nx_k
  )
}

summarize_cod_win_prop_after_k_wins_i_rounds <- function(k) {
  probs_nx_k <- crossing(
    n_rounds = 7L:11L,
    round = 3L:11L,
    win_series = c(TRUE, FALSE),
    win_round = c(TRUE, FALSE)
  ) |> 
    filter(round >= (!!k + 1L)) |> 
    mutate(
      is_valid = case_when(
        win_series ~ TRUE,
        (n_rounds - !!k - 6L) >= 0L ~ TRUE,
        TRUE ~ FALSE
      )
    ) |> 
    filter(is_valid) |> 
    select(-is_valid) |> 
    mutate(
      naive_prop = ifelse(win_series, 6L / n_rounds, (n_rounds - 6L) / n_rounds),
      notional_prop = (ifelse(win_series, 6L, n_rounds - 6L) - k) / (n_rounds - k),
      across(c(naive_prop, notional_prop), ~ifelse(win_round, .x, 1 - .x)),
      ms_prop = map2_dbl(
        n_rounds, naive_prop, 
        ~simulate_post_streak_prob(n = ..1, k = !!k, p = ..2)
      ),
      across(
        ms_prop,
        ~ifelse(win_round, .x, naive_prop + (naive_prop - .x))
      )
    )
  
  postprocess_round_streaks(
    k = k,
    probs_nx_k = probs_nx_k,
    round
  )
}

cod_round_win_prop_after_k_wins <- tibble(win_streak = 2L:5L) |>
  mutate(
    data = map(win_streak, summarize_cod_win_prop_after_k_wins)
  ) |>
  unnest(data)
cod_round_win_prop_after_k_wins |> filter(win_round, notional_p_value < 0.05)
cod_round_win_prop_after_k_wins |> filter(win_round, ms_p_value < 0.05)

cod_round_win_prop_after_k_wins_i_rounds <- tibble(win_streak = 2L:3L) |>
  mutate(
    data = map(win_streak, summarize_cod_win_prop_after_k_wins_i_rounds)
  ) |>
  unnest(data)
cod_round_win_prop_after_k_wins_i_rounds |> filter(win_round, notional_p_value < 0.05)
```

```{r}
#| label: cod_round_win_prop_after_k_wins-table
cod_round_win_prop_after_k_wins |> 
  filter(win_streak == 2L) |> 
  mutate(
    across(
      matches('p_value'), 
      ~case_when(
        .x <= 0.01 ~ '<=0.01', 
        TRUE ~ sprintf('%.2f', .x)
      )
    ),
    prop_lab = scales::percent(prop, accuracy = 0.1),
    notional_lab = sprintf('%.1f%% (%s)', round(100 * notional_prop, 1), notional_p_value),
    ms_lab = sprintf('%.1f%% (%s)', round(100 * ms_prop, 1), ms_p_value)
  ) |>
  arrange(n_rounds, win_series, win_round) |> 
  transmute(
    n_rounds,
    across(win_series, ~ifelse(.x, 'yes', 'no')),
    win_round,
    total,
    prop_lab,
    notional_lab,
    ms_lab
  ) |>
  group_split(win_round) |> 
  walk(
    function(df) {
      message(sprintf('win_round=%s, win_series=%s', df$win_round[[1]], df$win_series[[1]]))
      df <- df |> 
        select(
          -c(win_round)
        ) |> 
        gt::gt() |> 
        gt::cols_label(
          n_rounds = 'r',
          win_series = 'Win series?',
          total = 'N',
          prop_lab = 'O',
          notional_lab = 'a',
          ms_lab = 'b'
        ) |> 
        gt::as_latex() |> 
        cat()
    }
  )

```

Given that people typically perceive streaks as beginning after the third success (or failure) at minimum [@carlson2007], I focus on streaks of three round wins.[^19] Table \ref{tbl:cod-pw3r-pl3r} compares the observed round win rate $P^{w,kr}$ following streaks of $k=3$ round wins given the series outcome ($r$ rounds and winner) with the notional and MS expected proportions, $\hat{P}^{w|kr}_0$ and $\hat{P}^{w|kr}_{MS}$ respectively.

[^19]: Three happens to also be a reasonable number for series that lasts at maximum 11 rounds.

```{=tex}
\begin{longtable}{rrrrrr}
\caption{Observed count and proportion of round wins immediately following round win streak of $k=3$, $N^{w|kr}$ and $P^{w|kr}$ respectively, given the length of the series ($r$ rounds) and the series winner. Additionally, the notional and MS expected proportions, $\hat{P}^{w|kr}_0$ and $\hat{P}^{w|kr}_{MS}$ respectively, and the $p$-values of binomial tests comparing the underlying notional and MS round win probabilities, $p^{w|kr}_0$ and $p^{w|kr}_{MS}$, to the probability implied by the observed proportion $p^{w|kr} = N^{w|kr} \times P^{w|kr}$.}
\label{tbl:cod-pw3r-pl3r} \\
\toprule
$r$ & \text{Win series?} & $N^{w|kr}$ & $P^{w|kr}$ & $\hat{P}^{w|kr}_0$ ($p$-value) & $\hat{P}^{w|kr}_{MS}$ ($p$-value) \\ 
\midrule
7 & yes & 209 & 74.6\% & 75.0\% (0.94) & 75.7\% (0.69) \\ 
8 & yes & 209 & 62.2\% & 60.0\% (0.53) & 61.9\% (0.94) \\ 
9 & yes & 193 & 51.8\% & 50.0\% (0.67) & 52.7\% (0.83) \\ 
10 & no & 60 & 13.3\% & 14.3\% (1.00) & 26.7\% (0.02) \\ 
10 & yes & 151 & 43.7\% & 42.9\% (0.87) & 44.7\% (0.87) \\ 
11 & no & 129 & 24.0\% & 25.0\% (0.84) & 30.8\% (0.11) \\ 
11 & yes & 150 & 40.0\% & 37.5\% (0.56) & 38.8\% (0.80) \\ 

\bottomrule
\end{longtable}
```
We observe that, with the exception of $\hat{P}^{w|kr}_{MS}$ when $r = 10$ and the eventual series loser is the team that wins after a streak of three round wins[^20], all binomial test $p$-values are greater than the typical $\alpha = 0.05$ threshold, implying that we cannot reject the null hypothesis that the expected notional and MS post-streak round win rates are different than the observed proportion. As noted before, the MS expectations may be unreliable for CoD SnD, so one is inclined to prefer the results of the notional binomial tests.

[^20]: This implies that the opponent, th eventual series winner, goes on to win the series 6-4 after going down 0-4. This is the only scenario in which one team wins four rounds consecutively but loses a best-of-11 series in 10 rounds.

When performing the same tests for streaks of two, four, and five, there is no case in which I can reject the binomial null hypothesis for the expected notional probability $p^{w|kr}_0$. The null hypothesis can be rejected for expected MS probability $p^{w|kr}_{MS}$ when $k = 4, r = [7, 8, 10]$ and $k = 5, r = [7, 8]$. Nonetheless, the usefulness of $p^{w|kr}_{MS}$ is questionable, as described before, and the sample sizes are smaller for these conditions (all under 20 instances, with the exception of $k = 4, r = [7, 8]$ with 70 and 39 observations respectively).

# References {.unnumbered}

::: {#refs}
:::
