---
title: "The Hot Hand Fallacy in Call of Duty Search and Destroy"
link-citations: true
authors:
  - name: Person One
    email: personone@domain.com
  - name: Person Two
    email: persontwo@domain.com
abstract: |
  Our research investigates patterns in round win percentages in professional Search and Destroy (SnD) matches of the popular first-person shooter game Call of Duty (CoD). First, we find evidence supporting the hypothesis that round win probability can be modeled as a constant across rounds in the series, although not at the naive 50%. Second, we examine the proportion of round wins immediately following a win streak. Given streak length, series length, and series winner, we find no evidence that post-streak win proportion is significantly different than expected, suggesting that the perception of momentum at the team level is deceiving. Wald-Wolfowitz run tests also fail to provide evidence for the hot hand phenomenon.
bibliography: references.bib
keywords:
  - esports
  - Call of Duty
  - hot hand
  - runs test
output:
  rticles::arxiv_article:
    includes:
      in_header: preamble.tex
---

```{r}
#| label: setup
#| include: false
#| echo: false
#| eval: true
knitr::opts_chunk$set(
  eval = FALSE,
  echo = FALSE,
  include = FALSE,
  message = FALSE,
  warning = FALSE
)
```

```{r}
#| label: setup-code
library(readr)
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)

## i annotate when i use these with ::
library(broom)
library(randtests)
library(withr)

cod_rounds <- read_csv('data/cod_rounds.csv')
```

```{r}
#| label: descriptive-cod-snd-stats
cod_n_series <- cod_rounds |> 
  distinct(series_id) |> 
  nrow()

cod_n_rounds <- cod_rounds |> 
  distinct(series_id, round) |> 
  nrow()

cod_o_win_prop <- cod_rounds |> 
  filter(is_offense) |> 
  count(win_round) |> 
  mutate(prop = n / sum(n)) |> 
  filter(win_round) |> 
  pull(prop)
cod_o_win_prop^2 * (1 - cod_o_win_prop)
cod_o_win_prop * (1 - cod_o_win_prop)^2

cod_o_win_prop_by_game <- cod_rounds |> 
  filter(is_offense) |> 
  count(game, win_round) |> 
  group_by(game) |> 
  mutate(prop = n / sum(n)) |> 
  ungroup() |> 
  filter(win_round) |> 
  select(game, n, prop)

cod_n_series_lan <- cod_rounds |> 
  filter(event |> str_detect('C[Hh][Aa][Mm][Pp]|Major')) |> 
  distinct(series_id) |> 
  nrow()

cod_win_series_prop <- cod_rounds |> 
  distinct(series_id, win_series, n_rounds) |> 
  count(n_rounds) |> 
  mutate(prop = n / sum(n)) |> 
  arrange(n_rounds)

cod_rounds |> 
  filter(round == 1) |> 
  count(game, team, is_offense) |> 
  group_by(game, team) |> 
  mutate(prop = n / sum(n)) |> 
  ungroup() |>
  filter(is_offense) %>% 
  arrange(-prop)
```

# Introduction

## Call of Duty Search and Destroy

Call of Duty (CoD), first released in 2003, is one of the most popular first-person shooter (FPS) video game franchises of all-time. The most popular mode in the competitive scene is "Search and Destroy" (SnD), in which one team tries to destroy one of two designated bomb sites on the map.[^1]

[^1]: SnD bears resemblance to "Bomb Defusal" in Counter-Strike and "Plant/Defuse" in Valorant, two other FPS games played in more popular professional leagues.

In professional CoD SnD, the two teams[^2] take turns playing offense and defense every round. The first to six round wins (best-of-11 round format) is declared the series winner.[^3] A round can end in one of four ways:

[^2]: In the 2020 season, teams played with five players each; in the 2021 and 2022 seasons, teams played with four players each.

[^3]: A short list of rules that govern timing follows.

-   Each round has a 90 second time limit, not counting the potential extension granted if a bomb is planted
-   A bomb plant takes five seconds. The timer resets if the player stops planting site prior to completing it.
-   A bomb defuse takes seven seconds. The timer resets if the player "drops" the bomb.
-   Once the bomb is planted, the round timer is set to 45 seconds.

1.  One team eliminates all members of the other team prior to a bomb plant. (Eliminating team wins.)
2.  The offensive team eliminates all members of the defensive team after a bomb plant. (Offense wins.)
3.  The defense defuses the bomb after a bomb plant. (Defense wins.)
4.  The offense does not make a plant by the time the round timer ends. (Defense wins.)

We adopt the terminology "series" to refer to a single best-of-11 matchup, so as to mirror the terminology of playoff series in professional leagues like the National Basketball Association (NBA), National Hockey League (NHL), and Major League Baseball (MLB). A "game" or a "match" in such leagues is analogous to a "round" of CoD SnD for the purposes of this paper. Note that SnD is not the only game mode played in competitive CoD.[^4] Teams play in a head-to-head, best-of-five format, where SnD is always played as the second and fifth game modes. The best-of-five matchup could also be called a "series", but since we analyze only the SnD games, we refer to the SnD games as series.

[^4]: Hardpoint has been played as the first and fourth game modes in a matchup. The third game mode was Domination in the 2020 season, and Control in the 2021 and 2022 seasons.

## Data

The data set consists of all SnD matches played in major tournaments and qualifiers during the past three years[^5], totaling 7,792 rounds across 852 series.[^6] Data has been collected in spreadsheets by community member "IOUTurtle".[^7]

[^5]: Since the release of "Call of Duty: Modern Warfare" in fall 2019, professional CoD has been orchestrated by the 12-franchise CoD League (CDL). The CDL has completed three year-long "seasons" as of August 2022.

[^6]: 288 of the series occur in major tournaments, which are considered to be more competitive than the qualifiers since they are played in person (COVID-permitting), eliminating randomness due to online latency and ping. The qualifiers are played online.

[^7]: Raw data: <https://linktr.ee/CDLArchive>. Cleaned data: <https://github.com/%7Bauthor%7D/%7Brepo%7D/blob/master/data/%7Bfile%7D.csv>.

CoD is fairly unique compared to other esports in that it runs on an annual lifecycle, with releases coming in the late fall. A new game under the same brand---"Call of Duty"---is published every year by a rotating set of developers. Each new game bears resemblance to past ones, often introducing relatively small variations ("improvements") to graphics, game modes, and other facets of gameplay. During the CDL era, the games released have been "Modern Warfare" (2020 season), "Cold War" (2021 season) and "Vanguard" (2022 season).

Sweeps (6-0 series) make up 4.7% of all series, while 6-5 series make up 23.7% of series. The observed offensive round win percentage[^8] across all rounds is $\bar{\tau}$ = 47.8%.[^9] Table \ref{tbl:o-win-prop-by-series-state} shows offensive round win percentages by series "state", i.e. the number of round wins by each team prior to an upcoming round. Offensive round win percentage is not quite constant, although never veers more than 10% from $\bar{\tau}$.

[^8]: We use the terms "percentage", "probability", and "proportion" synonymously throughout this paper when discussing round win rates. For the most part, we restrict "percentage" usage to observed values and "probability" usage to expected values, and we use "proportion" to refer to either observed or expected values.

[^9]: Offensive round win percentage has been nearly constant across the three games during the CDL era: 47.2% in Modern Warfare; 47.9% in Cold War; and 48.1% in Vanguard

```{=tex}
\begin{table}

\caption{Offensive round win percentage (\%) for the upcoming round, given both the offensive and defensive team's prior number of round wins. Numbers in parentheses indicate sample sizes.}

\centering
\begin{tabular}{crrrrrr}
\toprule
& \multicolumn{6}{c}{Offense round wins} \\ 
\cmidrule(lr){2-7}
Defense round wins & 0 & 1 & 2 & 3 & 4 & 5 \\ 
\midrule

0 & 47.8 (852) & 46.6 (408) & 43.1 (216) & 43.5 (115) & 43.3 (67)  & 40.5 (37)  \\
1 & 48.6 (444) & 49.3 (418) & 51.5 (309) & 43.4 (205) & 43.3 (120) & 39.4 (99)  \\
2 & 52.8 (218) & 48.9 (305) & 48.9 (315) & 46.6 (262) & 48.7 (189) & 42.1 (133) \\
3 & 54.5 (123) & 46.0 (200) & 49.6 (250) & 45.6 (248) & 44.4 (214) & 44.8 (174) \\
4 & 56.9 (65)  & 54.5 (145) & 47.2 (193) & 44.7 (228) & 55.2 (221) & 50.5 (208) \\
5 & 47.4 (38)  & 49.4 (83)  & 47.1 (136) & 50.9 (175) & 45.2 (177) & 46.0 (202) \\

\bottomrule
\end{tabular}

\label{tbl:o-win-prop-by-series-state}

\end{table}
```
The professional competition format balances the frequency with which teams play offense or defense to start a series, to the extent possible. Thus, win percentages in certain states are subjected to minimal selection bias, i.e. bias due to better teams playing defense more frequently.[^10]

[^10]: At tournaments, higher-seeded ("better") teams choose whether they want to start on offense in the SnD series, played as the second or fifth game mode of the best-of-five matchup. Their opponent is assigned to start on offense in the SnD slot not chosen. Anecdotally, the better team tends to choose to play defense in the first round of the SnD series played as the second game mode in the best-of-five matchup, although this choice is not consistent across teams, or even with the same team over time.

Outside of tournaments, teams play each other twice throughout the season, constituting qualifiers. Each team gets to play the higher-seeded role once in their head-to-head matchups with a given team.

This format has minimized the range of rates at which teams start series playing offense. The highest rate at which a team has started their series playing defense across all tournament and qualifier SnD series in a single season is 66% (31 of 47). The lowest is 38% (16 of 42).

# Literature review

There have been a handful of studies of the distribution of games played in a series of a professional sport. Most assume a constant probability $\phi$ of a given team winning a game in the series, regardless of the series state. Mosteller [-@mosteller1952] observed that the American League had dominated the National League in MLB World Series matchups through 1952, implying that games should not be modeled with a constant $\phi_0 = 0.5$. Mosteller proposed three approaches for identifying the optimal constant probability value of the stronger team in the World Series, finding $\hat{\phi} \approx 0.65$ in each case: (1) solve for $\phi$ from the observed average number of games won by the loser of the series, which he called the "method of moments" approach; (2) maximize the likelihood that the sample would have been drawn from a population in which the probability of a team winning a game is constant across the series (i.e. maximum likelihood), and (3) minimize the chi-squared goodness of fit statistic $\chi^2$ as a function of $\phi$.

Chance [-@chance2020] re-examined the constant probability notion in the MLB World Series (1923--2018), the NBA Finals (1951--2018), and the NHL Stanley Cup (1939--2018). Chance found strong evidence against the null hypothesis of $\phi_0 = 0.5$ in the MLB and NHL championship series when applying Mosteller's first and second methods.

The concept of momentum goes hand-in-hand with a discussion of the nature of series outcomes.[^11] Two opposing fallacies are observed in the context of momentum: the "gambler's fallacy" (negative recency) and the "hot hand fallacy" (positive recency). Per Ayton et al. [-@ayton2004], negative recency is "the belief that, for random events, runs of a particular outcome ... will be balanced by a tendency for the opposite outcome", while positive recency is the expectation of observing future results that match recent results.

[^11]: We often use "streaks" and momentum interchangeably, but as Steeger et al. [-@steeger2021] note, momentum implies dependence between events, whereas streaking does not.

Studying both player streaks and team streaks in basketball, in both observational and controlled settings, Gilovich et al. [-@gilovich1985], henceforth GVT, did not find evidence for the hot hand phenomenon. However, Miller and Sanjurjo [-@miller2018], henceforth MS, provided a framework for quantifying streak selection bias, which effectively works in the manner posited by the gambler's fallacy. Specifically, MS stated that a "bias exists in a common measure of the conditional dependence of present outcomes on streaks of past outcomes in sequential data" implying that, under i.i.d. conditions, "the proportion of successes among the outcomes that immediately follow a streak of consecutive successes is expected to be strictly less than the underlying (conditional) probability of success". When applying streak selection bias to GVT's data, MS came to the opposite conclusions as GVT.

Other research has borrowed techniques from the field of quality control, such as identifying unlikely sequences of events with the Wald-Wolfowitz runs test (Peel and Clauset 2015, Steeger et al. 2021). Peel and Clauset found no evidence for unlikely sequences of scoring events in the NHL, College Football, and National Football League, although did in the NBA. As a check on their entropy approach to momentum identification, Steeger et al. found several NHL teams with sequences of wins in the 2018-2019 regular season that violated the Wald-Wolfowitz null hypothesis.

## Our contribution

Despite the plethora of existing research on games played in a series and momentum in sports, these topics have yet to be investigated heavily, if at all, in esports. Work has been done to examine intra-round, second-by-second win probability in other FPS titles such as Counter-Strike [@xenopoulos2022] and Valorant [@derover2021], both of which are round-based like CoD SnD. However, considering Counter-Strike and Valorant specifically, research on round-level trends seems non-existent, perhaps for one of several reasons:

1.  Both have economic aspects that can create clear advantages for one team in a round, given how prior rounds played out. CoD has no such equivalent, except for perhaps "score streaks", which infrequently occur.
2.  Teams play either offense or defense for many consecutive rounds. (Counter-Strike has a 15-15-1 format and Valorant has a 12-12-1 format.) On the other hand, teams in CoD SnD rotate attacking and defending roles every round, minimizing the effect of map asymmetry on streaks of wins.[^12]
3.  Both Counter-Strike and Valorant have overtime rules---a team must win by two rounds---which can make end-of-series sequences difficult to model properly. CoD SnD does not have overtime rules.

[^12]: Professional CoD has a set of maps (at least three, depending on the season) on which teams choose to play SnD. Most are asymmetric, introducing slight advantages for the offense or defense that do not exist on other maps. Counter-Strike and Valorant also have their own sets of maps for their analogous SnD modes.

To our knowledge, there is no existing public statistical research on the CoD SnD format, beyond descriptive analysis on social media. While intra-round trends may be more directly applicable to teams looking for an advantage on their competition, broader investigation of a concept like the hot-hand fallacy in a sport where it has not yet been investigated---particularly one that is less subjected to factors that may be difficult to control for, e.g. weather---should be useful as an inspiration for future researchers.

# Methodology

## Distribution of rounds played

In a best-of-$s$ format, assuming a constant round win probability $\phi$ (where $0 \leq \phi \leq 1$) for a given team, the expected proportion of series ending in $r$ rounds (where $r \leq s$) is given by Equation \ref{eq:series-length}.

```{=tex}
\begin{equation}\label{eq:m}
m = \frac{s + 1}{2}.
\end{equation}
```
```{=tex}
\begin{equation}\label{eq:series-length}
\hat{\Phi}(r) = \frac{(r - 1)!}{(m - 1)!(s - r)!}(\phi^{m}(1 - \phi)^{r - m} + \phi^{r - m}(1 - \phi)^m).
\end{equation}
```
For example, assuming $\phi = 0.5$, the probability of a series ending in exactly nine rounds in CoD SnD, where $s=11$ (implying $m=6$), is

$$
\hat{\Phi}(9) = \frac{(9 - 1)!}{(6 - 1)!(11 - 9)!}(0.5^{6}(1 - 0.5)^{9 - 6} + 0.5^{9 - 6}(1 - 0.5)^6) = 56 (0.5^9 + 0.5^9) = 0.21875.
$$

To evaluate the constant round win probability hypothesis, we can compute the test statistic

```{=tex}
\begin{equation}\label{eq:chi-squ}
\chi^2 = \sum_{r \in R} \frac{(\Phi(r) - \hat{\Phi}(r))^2}{\hat{\Phi}(r)}
\end{equation}
```
where $\hat{\Phi}(r)$ and $\Phi(r)$ represent the expected and observed round win proportions respectively, and where $R = [6, 7, 8, 9, 10, 11]$ for CoD SnD. The null hypothesis is that the test statistic is chi-squared distributed, where the number of degrees of freedom is $\vert R \vert -1 = 5$ for Cod SnD. We use a significance level of $\alpha = 0.05$ for this test and all others in this paper.

## Momentum

### MS post-streak probability

Let us now consider round win proportions immediately following a streak of $k$ wins, given that the series lasts $r$ rounds. Removing our knowledge of a streak, we might expect the win proportion for a round to be

```{=tex}
\begin{equation}\label{eq:pwr}
\hat{P}_0(\text{win} | r) = \hat{P}^{+|r}_0 := \begin{cases} 
\frac{m}{r} & \text{team wins series}, \\ 
\frac{r - m}{r} & \text{team loses series},
\end{cases}
\end{equation}
```
for $r \in R$ and $m$ from Equation \ref{eq:m}.

As implied by MS's Theorem 1, we should expect the proportion of round wins immediately following a streak of $k$ rounds wins for a series ending in $r$ rounds, $\hat{P}^{+|k,r}_{MS}$, to be strictly less than the observed proportion of round wins given the series length, but not given knowledge of the potential existence of a streak, $P^{+|r}$, i.e. the analogue of the expected proportion in Equation \ref{eq:pwr} for the observed data. However, our context is fundamentally different from that of MS, who evaluate data in controlled settings, unaffected by opposition.[^13]

[^13]: GVT also perform statistical tests on shots from players in live games, i.e. "observational" data, but they note that their findings are likely affected by player shot selection influenced by the opposing team's defensive strategy. This is similar to our setting.

The number of trials is fixed in their experimental designs; in CoD SnD, however, the number of rounds played is determined as a function of the max number of possible rounds ($s$) and whether or not the team wins the series. The single round win percentage in CoD SnD---analogous to a player's shooting percentage in MS's analysis of basketball players---is not independent of the opponent. Consequently, the i.i.d. assumption of MS's theorem regarding post-streak success may or may not be valid. Nonetheless, we still leverage $\hat{P}^{+|k,r}_{MS}$ as a reference for our results.

### "Notional" post-streak probability

Let us consider another form of the expected proportion of rounds won immediately after a streak of $k$ round wins in a best-of-$s$ series ending in $r$ rounds: the "notional" proportion

```{=tex}
\begin{equation}\label{eq:pwkr}
\hat{P}_0(\text{win} | k, r) = \hat{P}^{+|k,r}_0 := \begin{cases}
\frac{m - k}{r - k} & \text{team wins series}, r > m; \\
1 & \text{team wins series}, r = m; \\
\frac{s - m - k}{r - k} & \text{team loses series}, r > m + k; \\
0 & \text{team loses series}, r = m + k,
\end{cases}
\end{equation}
```
where $r \in R$ and $m$ from Equation \ref{eq:m}.

We can perform a two-tailed binomial test to evaluate the null hypothesis $\omega = \omega_0$ for the observed proportion of successes, $\omega$, and a null proportion, $\omega_0$ (where $0 \leq \omega_0 \leq 1$). The probability of exactly $r^+$ successes (and $r^- = r - r^+$ failures) in a sample of $r$ trials is

```{=tex}
\begin{equation}\label{eq:binom}
\binom {r}{r^+} \omega^{r^+}(1-\omega)^{r^-},
\end{equation}
```
where $\omega = \frac{r^+}{r}$ is the observed proportion of successes. If we can reject the null hypothesis---treating $P^{+|k,r}$ and $\hat{P}^{+|k,r}_0$ as $\omega$ and $\omega_0$ respectively---then we might consider team momentum plausible. Since we perform this test for many combinations of $k$ and $r$, we adjust the $p$-values with the Benjamini and Yekutieli (BY) false discovery procedure [-@benjamini2001].

As a reference, we can perform the same binomial test for MS's proportion, $\hat{P}^{+|k,r}_{MS}$, which adjusts for streak selection bias. However, given the caveats mentioned before regarding the application of MS's theorem to our setting, the results of such binomial tests should be heeded with caution.

We can further decompose Equation \ref{eq:pwkr} by the round $i$ (where $k < i \leq r$) in which the streak of length $k$ carries into.

```{=tex}
\begin{equation}\label{eq:pwkri}
\hat{P}_0(\text{win} | k, r, i) = \hat{P}^{+|k,r,i}_0 := \begin{cases}
\frac{m - k}{r - k} & \text{team wins series}, i \neq r; \\
1 & \text{team wins series}, i = r; \\
\frac{s - m - k}{r - k} & \text{team loses series}, i \neq r, r > m + k; \\
0 & \text{team loses series}, i = r.
\end{cases}
\end{equation}
```
Again, we can apply a binomial test to evaluate the hypothesis that the expected proportion, $\hat{P}^{+|k,r,i}_0$, is equal to the observed proportion, $P^{+|k,r,i}$.

### Wald-Wolfowitz runs test

We can attempt to detect the hot hand phenomenon more broadly by focusing on non-random sequences identified by the Wald-Wolfowitz runs test. Note that the probability of success and failure need not be equal; thus, we can incorporate our findings regarding constant round win probability, specifying that the probability of success is $\phi_2 = 0.575$.

Under the null hypothesis for the Wald-Wolfowitz runs test, the number of runs in a sequence of $r$ trials is a random variable that can take on values $+$ or $-$ and arrive at $r^+$ successes (and $r^-$ failures), with the following mean $\mu_r$ [^14] and variance $\sigma_r^2$:

[^14]: We calculate $\mu$ and $\sigma^2$ for each number of rounds $r \in R$.

```{=tex}
\begin{equation}\label{eq:ww}
\mu_r = \frac{2r^{+}r^{-}}{r} + 1, \sigma_r^2 = \frac{(\mu_r-1)(\mu_r-2)}{r-1}.
\end{equation}
```
The null hypothesis is that the test statistic,

```{=tex}
\begin{equation}\label{eq:wwz}
z_r = \frac{r^* - \mu_r}{\sigma_r},
\end{equation}
```
where $r^*$ is the number of runs, is normally distributed.

We can subset the observed series sequences, $\zeta_j$, to those that violate the null hypothesis for the runs test and perform a test of equal proportions, where the null hypothesis is that the observed proportion of a sequence relative to all possible sequences, $P^{\zeta}_j$ ( $\frac{N^\zeta_j}{\sum_j N^\zeta }$), is equal to the expected proportion of the sequence relative to all possible sequences, $\hat{P}^\zeta_j$. The test statistic is

```{=tex}
\begin{equation}\label{eq:prop}
z^\zeta_j = \frac{P^\zeta_j - \hat{P}^\zeta_j}{\sqrt{P^\zeta_{j,\delta} (1 - P^\zeta_{j,\delta}) (1 / N^\zeta_j + 1 / \hat{N}^\zeta_j)  } }
\end{equation}
```
where $N^\zeta_j$ and $\hat{N}^\zeta_j$ respectively represent the observed and expected number of occurrences of sequence $\zeta_j$, and where $P^\zeta_{j,\delta} = (P^\zeta_j - \hat{P}^\zeta_j) / ({N^\zeta_j - \hat{N}^\zeta_j})$. The null hypothesis is that the test statistic is normally distributed. If we can reject the null hypothesis for such sequences, we can build an argument in support of the hot hand effect.[^15]

[^15]: As with the binomial tests for the notional and MS post-streak proportions, we adjust the $p$-values with the BY correction.

# Results

## Distribution of rounds played

Using Equation \ref{eq:chi-squ}, we find that $\chi^2 = 16.0$ ($p$-value of \<0.01) for $\phi_0 = 0.5$. Thus, we can comfortably reject the constant probability hypothesis for the naive $\phi_0 = 0.5$.

```{r}
#| label: cod_series_outcome_prop
MAX_COD_ROUND <- 11
COD_WIN_THRESHOLD <- 6
prob_of_series_lasting_r_rounds <- function(r, p = 0.5, s) {
  m <- as.integer((s + 1) / 2)
  (factorial(r - 1) / (factorial(m - 1) * factorial(r - m))) * (p^m * (1 - p)^(r - m) + p^(r - m) * (1 - p)^m)
}

expected_series_streaks_of_outcomes <- function(m, n) {
  factorial(m + n) / (factorial(m) * factorial(n))
}

# https://raw.githubusercontent.com/dgrtwo/splittestr/master/R/vectorized-prop-test.R
vectorized_prop_test_approx <- function(a, b, c, d) {
  n1 <- a + b
  n2 <- c + d
  n <- n1 + n2
  p <- (a + c) / n
  E <- cbind(p * n1, (1 - p) * n1, p * n2, (1 - p) * n2)
  
  x <- cbind(a, b, c, d)
  
  DELTA <- a / n1 - c / n2
  YATES <- pmin(.5, abs(DELTA) / sum(1 / n1 + 1 / n2))
  
  STATISTIC <- rowSums((abs(x - E) - YATES)^2 / E)
  PVAL <- pchisq(STATISTIC, 1, lower.tail = FALSE)
  PVAL
}

vectorized_prop_test_exact <- function(a, b, c, d) {
  sapply(seq_along(a), function(i) {
    fisher.test(cbind(c(a[i], c[i]), c(b[i], d[i])))$p.value
  })
}

vectorized_prop_test <- function(x1, n1, x2, n2, conf.level = 0.95) {
  a <- x1
  b <- n1 - x1
  c <- x2
  d <- n2 - x2
  
  # if any values are < 20, use Fisher's exact test
  exact <- (a < 20 | b < 20 | c < 20 | d < 20)
  
  pvalue <- rep(NA, length(a))
  
  if (any(exact)) {
    pvalue[exact] <- vectorized_prop_test_exact(a[exact], b[exact], c[exact], d[exact])
  }
  if (any(!exact)) {
    pvalue[!exact] <- vectorized_prop_test_approx(a[!exact], b[!exact], c[!exact], d[!exact])
  }
  
  mu1 <- a / (a + b)
  mu2 <- c / (c + d)
  
  ## confidence interval
  alpha2 <- (1 - conf.level) / 2
  DELTA <- mu2 - mu1
  WIDTH <- qnorm(alpha2)
  alpha <- (a + .5) / (a + b + 1)
  beta <- (c + .5) / (c + d + 1)
  
  n <- n1 + n2
  YATES <- pmin(.5, abs(DELTA) / sum(1 / n1 + 1 / n2))
  
  z <- qnorm((1 + conf.level) / 2)
  WIDTH <- z * sqrt(mu1 * (1 - mu1) / n1 + mu2 * (1 - mu2) / n2)
  
  tibble(
    estimate = DELTA,
    conf.low = pmax(DELTA - WIDTH, -1),
    conf.high = pmin(DELTA + WIDTH, 1),
    p.value = pvalue
  )
}

cod_actual_round_streaks <- cod_rounds |> 
  filter(round <= MAX_COD_ROUND) |> 
  filter(win_series) |> 
  mutate(across(win_round, as.integer)) |> 
  group_by(series_id) |> 
  summarize(
    wins = max(cumu_w),
    losses = max(cumu_l),
    ws = paste0(win_round, collapse = '-')
  ) |> 
  ungroup() |> 
  mutate(n_rounds = wins + losses) |> 
  unite(
    record, wins, losses, sep = '-'
  ) |>
  count(record, n_rounds, ws, sort = TRUE) |> 
  mutate(prop = n / sum(n))

summarize_cod_streaks <- function(p = 0.5) {
  
  expected_round_streaks <- tibble(
    n_rounds = COD_WIN_THRESHOLD:MAX_COD_ROUND
  ) |> 
    mutate(
      series_prop = map_dbl(n_rounds, ~prob_of_series_lasting_r_rounds(.x, s = MAX_COD_ROUND, p = !!p)),
      n_expected_series_streaks = map_dbl(n_rounds, ~expected_series_streaks_of_outcomes(COD_WIN_THRESHOLD, .x - COD_WIN_THRESHOLD))
    ) |> 
    transmute(
      n_rounds,
      series_prop,
      prop = series_prop / n_expected_series_streaks
    )
  
  round_streaks <- full_join(
    cod_actual_round_streaks |> 
      rename_with(~sprintf('%s_actual', .x), c(n, prop)),
    expected_round_streaks |> 
      rename_with(~sprintf('%s_expected', .x), prop),
    by = 'n_rounds'
  )
  
  round_streak_prop <- round_streaks |> 
    drop_na() |> 
    mutate(
      prop_diff = prop_actual - prop_expected,
      total_actual = sum(n_actual),
      n_expected = round(prop_expected * total_actual),
      p = vectorized_prop_test(n_actual, total_actual, n_expected, total_actual)
    ) |> 
    select(-total_actual) |> 
    unnest_wider(p) |> 
    arrange(p.value)
  
  series_outcomes <- full_join(
    cod_actual_round_streaks |> 
      group_by(record, n_rounds) |> 
      summarize(
        across(n, sum)
      ) |> 
      ungroup() |> 
      mutate(prop = n / sum(n)) |> 
      rename_with(~sprintf('%s_actual', .x), c(n, prop)),
    expected_round_streaks |> 
      select(n_rounds, prop_expected = series_prop),
    by = 'n_rounds'
  )
  
  series_outcome_prop <- series_outcomes |> 
    drop_na() |> 
    mutate(
      prop_diff = prop_actual - prop_expected,
      total_actual = sum(n_actual),
      n_expected = round(prop_expected * total_actual),
      p = vectorized_prop_test(n_actual, total_actual, n_expected, total_actual)
    ) |> 
    select(-total_actual) |> 
    unnest_wider(p) |> 
    arrange(p.value)
  
  list(
    rounds = round_streak_prop,
    series = series_outcome_prop
  )
}

cod_streaks_naive_res <- summarize_cod_streaks(p = 0.5)
```

```{r}
#| label: cod_series_outcomes_naive_chi
generate_chi_label <- function(statistic, p.value) {
  sprintf(
    '%.1f (%s)', 
    statistic, 
    ifelse(p.value <= 0.01, '<=0.01', as.character(round(p.value, 2)))
  )
}

perform_series_chi_test <- function(series) {
  chisq.test(
    series$n_actual, 
    p = series$prop_expected
  ) |> 
    broom::tidy()
}

cod_series_outcomes_naive_chi <- cod_streaks_naive_res$series |> 
  perform_series_chi_test()
```

```{r}
#| label: cod-alternative-to-p=0.5-method-prep
min_p1 <- 0.5
max_p1 <- 0.7
interval_p1 <- 0.0025
cod_ps <- tibble(p = seq(min_p1, max_p1, by = 0.0025))
```

```{r}
#| label: cod-alternative-to-p=0.5-method-1
## p. 365 on https://math.mit.edu/classes/18.095/2016IAP/lec9/Sports_Mosteller1952_WorldSeries.pdf
cod_prob_of_series_lasting_r_rounds <- function(r, p) {
  prob_of_series_lasting_r_rounds(r = r, p = p, s = 11)
}

theoretical_cod_series_length <- function(p) {
  6 * cod_prob_of_series_lasting_r_rounds(r = 6, p = p) +
    7 * cod_prob_of_series_lasting_r_rounds(r = 7, p = p) +
    8 * cod_prob_of_series_lasting_r_rounds(r = 8, p = p) +
    9 * cod_prob_of_series_lasting_r_rounds(r = 9, p = p) +
    10 * cod_prob_of_series_lasting_r_rounds(r = 10, p = p) +
    11 * cod_prob_of_series_lasting_r_rounds(r = 11, p = p)
}

cod_rounds_per_series <- cod_streaks_naive_res$series |>
  summarize(
    actual = sum(n_rounds * prop_actual),
    expected = sum(n_rounds * prop_expected)
  )

cod_theoretical_series_lengths1 <- cod_ps |>
  mutate(
    theoretical_series_length = map_dbl(p, theoretical_cod_series_length),
    diff = theoretical_series_length - cod_rounds_per_series$actual
  )

cod_theoretical_p1 <- cod_theoretical_series_lengths1 |>
  slice_min(abs(diff), n = 1) |>
  pull(p)
```

```{r}
#| label: cod-alternative-to-p=0.5-method-2
pluck_cod_n <- function(.n_rounds) {
  cod_streaks_naive_res$series |>
    filter(n_rounds == .n_rounds) |>
    pull(n_actual)
}

cod_n6 <- pluck_cod_n(6)
cod_n7 <- pluck_cod_n(7)
cod_n8 <- pluck_cod_n(8)
cod_n9 <- pluck_cod_n(9)
cod_n10 <- pluck_cod_n(10)
cod_n11 <- pluck_cod_n(11)

## This adjustment is something that is not done in the paper. We do it here to reduce
##   the magnitude of the exponents.
cod_pm <- pmin(cod_n6, cod_n7, cod_n8, cod_n9, cod_n10, cod_n11)
cod_n6adj <- round(10 * cod_n6 / cod_pm)
cod_n7adj <- round(10 * cod_n7 / cod_pm)
cod_n8adj <- round(10 * cod_n8 / cod_pm)
cod_n9adj <- round(10 * cod_n9 / cod_pm)
cod_n10adj <- round(10 * cod_n10 / cod_pm)
cod_n11adj <- round(10 * cod_n11 / cod_pm)

maximize_cod_series_p <- function(p) {
  cod_prob_of_series_lasting_r_rounds(r = 6, p = p)^cod_n6adj *
    cod_prob_of_series_lasting_r_rounds(r = 7, p = p)^cod_n7adj *
    cod_prob_of_series_lasting_r_rounds(r = 8, p = p)^cod_n8adj *
    cod_prob_of_series_lasting_r_rounds(r = 9, p = p)^cod_n9adj *
    cod_prob_of_series_lasting_r_rounds(r = 10, p = p)^cod_n10adj *
    cod_prob_of_series_lasting_r_rounds(r = 11, p = p)^cod_n11adj
}

cod_theoretical_series_lengths2 <- cod_ps |>
  mutate(
    theoretical_series_length = map_dbl(p, maximize_cod_series_p)
  )

cod_theoretical_p2 <- cod_theoretical_series_lengths2 |>
  slice_max(theoretical_series_length, n = 1) |>
  pull(p)
```

```{r}
#| label: cod-alternative-to-p=0.5-method-3
summarize_cod_series_chi <- function(p) {
  summarize_cod_streaks(p) |> 
    pluck('series') |> 
    perform_series_chi_test()
}

cod_theoretical_series_lengths3 <- cod_ps |> 
  filter(p >= 0.56, p < 0.585) |> 
  mutate(
    chi_squ = map_dbl(
      p,
      ~summarize_cod_series_chi(.x) |> 
        pluck('statistic')
    )
  )

cod_theoretical_p3 <- cod_theoretical_series_lengths3 |> 
  slice_min(chi_squ, n = 1) |> 
  pull(p)
```

```{r}
#| label: cod-alternatives-to-p=0.5
cod_streaks_other_res <- tibble(
  p = c(0.5, cod_theoretical_p1, cod_theoretical_p2, cod_theoretical_p3),
  idx = 0L:3L,
  name = c('0. Naive', '1. Method of moments', '2. Maximum likelihood', '3. Minimum Chi-square')
) |> 
  mutate(
    series_streaks = map(
      p,
      ~summarize_cod_streaks(.x) |> 
        pluck('series') 
    ),
    chi_squ = map(
      series_streaks,
      ~.x |> 
        perform_series_chi_test() |> 
        select(statistic, p.value)
    )
  )
```

Table \ref{tbl:prob-series-lasts-r-rounds} shows the expected proportion of series ending in $r$ rounds given $\phi_0 = 0.5$, $\hat{\Phi}_0(r)$, as well as the observed proportions, $\Phi(r)$.

```{=tex}
\begin{table}
\caption{The observed frequencies of CoD SnD series ending in $r$ rounds shown as a count $N(r)$ and as a proportion $\Phi(r)$ of all series ($\sum_{r \in R} N(r)$, where $r \in R = [6, 7, 8, 9, 10, 11]$). Additionally, the expected proportion, $\hat{\Phi}_0(r)$, under the assumption that each team has a constant round win probability $\phi_0 = 0.5$.}

\centering
\begin{tabular}{rrrr}
\toprule
$r$ & $N(r)$ & $\Phi(r)$ & $\hat{\Phi}_0(r)$ \\ 
\midrule

6 & 40 & $4.7\%$ & $3.1\%$ \\ 
7 & 101 & $11.9\%$ & $9.4\%$ \\ 
8 & 141 & $16.5\%$ & $16.4\%$ \\ 
9 & 185 & $21.7\%$ & $21.9\%$ \\ 
10 & 183 & $21.5\%$ & $24.6\%$ \\ 
11 & 202 & $23.7\%$ & $24.6\%$ \\ 

\bottomrule
\end{tabular}

\label{tbl:prob-series-lasts-r-rounds}

\end{table}
```
Table \ref{tbl:mosteller-methods-results} shows the alternate values for the constant round win probability assumption, each approximately equal to 0.575. When applying Equation \ref{eq:chi-squ}, each results in a $\chi^2$ value for which we cannot reject the constant probability null hypothesis.

```{=tex}
\begin{table}

\caption{Alternate estimates of the constant round win probability ($\phi$) and their corresponding $\chi^2$ values, applying the three methods suggested by Mosteller to CoD SnD. Additionally, the naive constant round win probability $\phi_0$ and its $\chi^2$.}

\centering
\begin{tabular}{lrr}
\toprule
Method & $\phi$ & $\chi^2$ ($p$-value) \\
\midrule

$\phi_0$: Naive & 0.5000 & 16.0 ($\leq$ 0.01) \\
$\phi_1$: Method of moments & 0.5725 & 3.6 (0.6) \\
$\phi_2$: Maximum likelihood & 0.5750 & 3.5 (0.62) \\
$\phi_3$: Minimum $\chi^2$ & 0.5775 & 3.5 (0.62) \\

\bottomrule
\end{tabular}

\label{tbl:mosteller-methods-results}

\end{table}
```
Table \ref{tbl:alternative-constant-ps} shows the expected proportions of series ending in $r$ rounds, $\hat{\Phi}_2(r)$, when re-applying Equation \ref{eq:series-length} for the maximum likelihood estimate $\phi_2 = 0.575$.[^16] We observe that $\hat{\Phi}_2(r)$ more closely matches $\Phi(r)$ than $\hat{\Phi}_0(r)$.

[^16]: The method of moments and minimum $\chi^2$ estimates for $\phi$ are omitted simply because the results would be nearly identical to those for the maximum likelihood estimate of $\phi$ (since they are all $\approx 0.575$).

```{=tex}
\begin{table}

\caption{The observed proportion of CoD SnD series ending in $r$ rounds, $\Phi(r)$, compared to the expected proportion, $\hat{\Phi}_2(r)$, under the maximum likelihood estimate, $\phi_2 = 0.575$. Additionally, the expected proportion, $\hat{\Phi}_0(r)$, under the naive assumption $\phi_0 = 0.5$ is repeated from Table \ref{tbl:prob-series-lasts-r-rounds} for comparison.}

\centering
\begin{tabular}{rrrr}
\toprule
$r$ & $\Phi(r)$ & $\hat{\Phi}_0(r)$ = 0.5 & $\hat{\Phi}_2(r)$ = 0.575 \\
\midrule

6 & 4.7\% & 3.1\% & 4.2\% \\
7 & 11.9\% & 9.4\% & 11.2\% \\
8 & 16.5\% & 16.4\% & 17.8\% \\
9 & 21.7\% & 21.9\% & 21.8\% \\
10 & 21.5\% & 24.6\% & 23.0\% \\
11 & 23.7\% & 24.6\% & 22.0\% \\

\bottomrule
\end{tabular}

\label{tbl:alternative-constant-ps}

\end{table}
```
The observation that $\hat{\Phi}_2(r)$ reasonably matches $\Phi(r)$ supports the implication from the failure to reject the null hypothesis for $\phi_2$ (shown in Table \ref{tbl:mosteller-methods-results}): the constant round win probability assumption can be valid for CoD SnD with the appropriate choice of $\phi$ ($\approx 0.575$).

## Momentum

### Post-streak probability

```{r}
#| label: cod_round_win_prop_after_k_wins
## http://keyonvafa.com/hot-hand/
get_post_streak_prob <- function(n, k, p = 0.5) {
  tosses <- rbinom(n, 1, p)
  runs <- rle(tosses)
  n_neg_after <- length(which(runs$values == 1 & runs$lengths >= k))
  n_pos_after <- sum(runs$lengths[which(runs$values == 1 & runs$lengths >= k)] - k)
  
  ## edge case
  if (n %in% cumsum(runs$lengths)[which(runs$values == 1 & runs$lengths >= k)]) {
    n_neg_after <- n_neg_after - 1
  }
  
  n_pos_after / (n_pos_after + n_neg_after)
}

simulate_post_streak_prob <- function(sims = 10000, seed = 42, ...) {
  withr::local_seed(seed)
  rerun(
    sims,
    get_post_streak_prob(...)
  ) |> 
    flatten_dbl() |> 
    mean(na.rm = TRUE)
}

vectorized_binom_test <- function(x, n, p, ...) {
  map_dfr(
    seq_along(x), 
    function(i) {
      res <- binom.test(x[i], n[i], p = p[i], ...)
      tibble(
        estimate = res$estimate,
        conf.low = res$conf.int[[1]],
        conf.high = res$conf.int[[2]],
        p.value = res$p.value
      )
    }
  )
}

cod_round_streaks <- cod_rounds |> 
  group_by(series_id, team) |> 
  mutate(
    won_prior_round1 = lag(win_round, n = 1, default = NA),
    won_prior_round2 = lag(win_round, n = 2, default = NA),
    won_prior_round3 = lag(win_round, n = 3, default = NA),
    won_prior_round4 = lag(win_round, n = 4, default = NA),
    won_prior_round5 = lag(win_round, n = 5, default = NA)
  ) |> 
  ungroup() |>
  select(
    series_id,
    team,
    is_offense,
    round,
    n_rounds,
    win_series,
    win_round,
    starts_with('won_prior_round')
  )

postprocess_round_streaks <- function(k, probs_nx_k, ...) {
  
  cols <- sprintf('won_prior_round%d', 2:k)
  col_syms <- syms(cols)
  cod_round_streaks_after_x <- cod_round_streaks |> 
    drop_na(!!!col_syms) |> 
    count(n_rounds, win_series, ..., win_round, won_prior_round1, !!!col_syms, sort = TRUE)
  
  cod_round_win_prop_after_z <- cod_round_streaks_after_x |> 
    filter(won_prior_round1, !!!col_syms)
  
  suppressMessages(
    cod_round_win_prop_after_z_with_probs <- cod_round_win_prop_after_z |> 
      group_by(win_series, n_rounds, ...) |> 
      mutate(
        total = sum(n),
        prop = n / total
      ) |> 
      ungroup() |> 
      select(n_rounds, win_round, win_series, ..., n, total, prop) |> 
      # filter(!win_round) |> 
      arrange(n_rounds, win_round, ...) |> 
      inner_join(
        probs_nx_k
      )
  )
  
  cod_round_win_prop_after_z_with_probs |> 
    mutate(
      notional_p_value = vectorized_binom_test(n, total, p = notional_prop)$p.value,
      ms_p_value = vectorized_binom_test(n, total, p = ms_prop)$p.value,
      notional_p_value_adj = notional_p_value |> p.adjust(method = 'BY')
    )
}

compute_probs_nx_k <- function(df, k) {
  df |> 
    mutate(
      is_valid = case_when(
        win_series ~ TRUE,
        (n_rounds - !!k - 6L) >= 0L ~ TRUE,
        TRUE ~ FALSE
      )
    ) |> 
    filter(is_valid) |> 
    select(-is_valid) |> 
    mutate(
      naive_prop = ifelse(win_series, 6L / n_rounds, (n_rounds - 6L) / n_rounds),
      notional_prop = (ifelse(win_series, 6L, n_rounds - 6L) - k) / (n_rounds - k),
      across(c(naive_prop, notional_prop), ~ifelse(win_round, .x, 1 - .x)),
      ms_prop = map2_dbl(
        n_rounds, naive_prop, 
        ~simulate_post_streak_prob(n = ..1, k = !!k, p = ..2)
      ),
      across(
        ms_prop,
        ~ifelse(win_round, .x, naive_prop + (naive_prop - .x))
      ),
      across(
        ms_prop,
        ~ifelse(is.nan(ms_prop), naive_prop, .x)
      )
    )
}

summarize_cod_win_prop_after_k_wins <- function(k) {
  probs_nx_k <- crossing(
    n_rounds = 7L:11L,
    win_series = c(TRUE, FALSE),
    win_round = c(TRUE, FALSE)
  ) |> 
    compute_probs_nx_k(k)
  
  postprocess_round_streaks(
    k = k,
    probs_nx_k = probs_nx_k
  )
}

init_cod_round_win_prop_after_k_wins <- tibble(win_streak = 2L:5L) |>
  mutate(
    data = map(win_streak, summarize_cod_win_prop_after_k_wins)
  ) |>
  unnest(data)

cod_round_win_prop_after_k_wins <- init_cod_round_win_prop_after_k_wins |> 
  filter(win_round) |> 
  mutate(
    notional_p_value_adj = notional_p_value |> p.adjust(method = 'BY'),
    ms_p_value_adj = ms_p_value |> p.adjust(method = 'BY')
  )

cod_round_win_prop_after_k_wins |> filter(win_round, notional_p_value_adj < 0.05)
cod_round_win_prop_after_k_wins |> filter(win_round, ms_p_value_adj < 0.05)
```

Given that people typically perceive streaks as beginning after the third success (or failure) at minimum [@carlson2007], we focus on streaks of three round wins. Table \ref{tbl:pwkr} compares the observed round win proportion, $P^{+|k=3,r}$, with the notional proportion, $\hat{P}^{+|k=3,r}$, following streaks of $k=3$ round wins. The MS proportion, $\hat{P}^{+|k=3,r}_{MS}$, is also shown as a reference.

```{=tex}
\begin{table}

\caption{For streaks of $k=3$ round wins in CoD SnD series ending in $r$ rounds, the observed count of rounds wins, $r^{+|k=3,r}$, and proportion of round wins, $P^{+|k=3,r}$, among $N^{k=3,r}$ chances ($P^{+|k=3,r} = r^{+|k=3,r} / N^{k=3,r}$). Additionally, the notional and MS expected proportions, $\hat{P}^{+|k=3,r}_0$ and $\hat{P}^{+|k=3,r}_{MS}$, respectively.
}

\centering
\begin{tabular}{rcrrrrr}
\toprule
$r$ & \text{Win series?} & $r^{+|k=3,r}$ & $N^{k=3,r}$ & $P^{+|k=3,r}$ & $\hat{P}^{+|k=3,r}_0$ & $\hat{P}^{+|=k=3,r}_{MS}$ \\ 
\midrule

7 & yes & 156 & 209 & 74.6\% & 75.0\% & 75.7\% \\ 
8 & yes & 130 & 209 & 62.2\% & 60.0\% & 61.9\% \\ 
9 & yes & 100 & 193 & 51.8\% & 50.0\% & 52.7\% \\ 
10 & no & 8 & 60 & 13.3\% & 14.3\% & 26.7\% \\ 
10 & yes & 66 & 151 & 43.7\% & 42.9\% & 44.7\% \\ 
11 & no & 31 & 129 & 24.0\% & 25.0\% & 30.8\% \\ 
11 & yes & 60 & 150 & 40.0\% & 37.5\% & 38.8\% \\ 

\bottomrule
\end{tabular}

\label{tbl:pwkr}

\end{table}
```
All binomial test $p$-values for both $\hat{P}^{+|k=3,r}_0$ and $\hat{P}^{+|k=3,r}_{MS}$ are found to be insignificant at the $\alpha = 0.05$ threshold, implying that we cannot reject the null hypothesis that the notional and MS post-streak round win proportions are different than the observed proportion for streaks of three.[^17] When performing the same tests for streaks of two, four, and five, there is no case in which we can reject the null hypothesis for $\hat{P}^{+|k,r}_0$. The null hypothesis can only be rejected in some extreme cases for $\hat{P}^{+|k,r}_{MS}$.[^18]

[^17]: $p$-values are not shown for readability purposes.

[^18]: These cases are: (1) $k = 4$, $r = 7$ ($N^{k,r} = 108$); (2) $k = 4$, $r = 10$ ($N^{k,r} = 45$); (3) $k = 5, r = 7$ ($N^{k,r} = 37$); (4) $k = 5, r = 8$ ($N^{k,r} = 4$). In each case, the team won the series.

```{r}
#| label: cod_round_win_prop_after_k_wins_i_rounds
summarize_cod_win_prop_after_k_wins_i_rounds <- function(k) {
  probs_nx_k <- crossing(
    n_rounds = 7L:11L,
    round = 3L:11L,
    win_series = c(TRUE, FALSE),
    win_round = c(TRUE, FALSE)
  ) |> 
    filter(round >= (!!k + 1L), n_rounds > round) |> 
    compute_probs_nx_k(k)
  
  postprocess_round_streaks(
    k = k,
    probs_nx_k = probs_nx_k,
    round
  )
}

init_cod_round_win_prop_after_k_wins_i_rounds <- tibble(win_streak = 3L) |>
  mutate(
    data = map(win_streak, summarize_cod_win_prop_after_k_wins_i_rounds)
  ) |>
  unnest(data) |> 
  select(-starts_with('ms'))

cod_round_win_prop_after_k_wins_i_rounds <- init_cod_round_win_prop_after_k_wins_i_rounds |>
  group_by(win_streak, win_round, n_rounds, round) |> 
  mutate(
    total = sum(n),
    prop = n / total
  ) |> 
  ungroup() |> 
  mutate(
    notional_p_value = vectorized_binom_test(n, total, p = notional_prop)$p.value
  ) |> 
  filter(win_round) |> 
  mutate(
    notional_p_value_adj = notional_p_value |> p.adjust(method = 'BY')
  )

# ## example of why we needed to recalculate prop... prop and notional_prop should sum to 1, grouped by win_streak and win_round
init_cod_round_win_prop_after_k_wins_i_rounds |>
  filter(win_streak == 3, n_rounds == 11, round == 10)

cod_round_win_prop_after_k_wins_i_rounds |>
  filter(win_streak == 3, n_rounds == 11, round == 10)
 
# ## 5-3?
# cod_round_win_prop_after_k_wins_i_rounds |> 
#   filter(win_streak == 2) |> 
#   filter(n_rounds == 11, round == 11)
```

Table \ref{tbl:pw3ri} shows the observed and notional proportions of round wins immediately following streaks of three round wins when explicitly accounting for the round index, $i$.[^19]

[^19]: We omit the MS proportions $\hat{P}^{+|k=3,r,i}_{MS}$ since sample sizes are small and results may or may not be valid due to the questionable i.i.d assumption.

```{=tex}
\begin{table}

\caption{For streaks of $k=3$ round wins in CoD SnD series ending in $r$ rounds, the observed count of rounds wins, $r^{+|k=3,r,i}$, and proportion of round wins, $P^{+|k=3,r,i}$, among $N^{k=3,r,i}$ chances, given the index of the round, $i$, immediately following the streak. Additionally, the notional proportion, $\hat{P}^{+|k=3,r,i}_0$. Table restricted to $r \in [10, 11]$, $i < r$, and $N^{k=3,r,i} \geq 10$.}

\centering
\begin{tabular}{rrcrrrr}
\toprule
$r$ & $i$ & \text{Win series?} & $r^{+|k=3,r,i}$ & $N^{k=3,r,i}$ & $P^{+|k=3,r,i}$ & $\hat{P}^{+|k=3,r,i}_0$\\ 
\midrule

10 & 7 & no & 2 & 13 & 15.4\% & 14.3\% \\ 
10 & 7 & yes & 11 & 13 & 84.6\% & 42.9\% \\ 
10 & 8 & no & 1 & 10 & 10.0\% & 14.3\% \\ 
10 & 8 & yes & 9 & 10 & 90.0\% & 42.9\% \\ 
11 & 5 & no & 9 & 13 & 69.2\% & 25.0\% \\ 
11 & 5 & yes & 4 & 13 & 30.8\% & 37.5\% \\ 
11 & 6 & no & 4 & 13 & 30.8\% & 25.0\% \\ 
11 & 6 & yes & 9 & 13 & 69.2\% & 37.5\% \\ 
11 & 7 & no & 3 & 10 & 30.0\% & 25.0\% \\ 
11 & 7 & yes & 7 & 10 & 70.0\% & 37.5\% \\ 
11 & 9 & no & 5 & 15 & 33.3\% & 25.0\% \\ 
11 & 9 & yes & 10 & 15 & 66.7\% & 37.5\% \\ 
11 & 10 & no & 5 & 10 & 50.0\% & 25.0\% \\ 
11 & 10 & yes & 5 & 10 & 50.0\% & 37.5\% \\ 

\bottomrule
\end{tabular}

\label{tbl:pw3ri}

\end{table}
```
```{r}
#| label: cod_round_arrangements
simulate_cod_round_arrangements <- function(p = 0.5, n_sims = 10000, seed = 42) {
  withr::local_seed(seed)
  w <- sample(c(0, 1), size = MAX_COD_ROUND * n_sims, replace = TRUE, prob = c(1 - p, p))
  m <- matrix(w, nrow = n_sims, ncol = MAX_COD_ROUND)
  df <- as_tibble(m)
  names(df) <- sprintf('%d', 1:MAX_COD_ROUND)
  df$i <- 1:nrow(df)
  
  sim_rounds <- tibble(
    i = rep(1:n_sims, each = MAX_COD_ROUND),
    r = rep(1:n_sims, times = MAX_COD_ROUND),
    w = w
  ) |> 
    group_by(i) |> 
    mutate(
      cumu_w = cumsum(w),
      cumu_l = cumsum(w == 0)
    ) |> 
    ungroup() |> 
    mutate(
      cumu_wl_max = ifelse(cumu_w > cumu_l, cumu_w, cumu_l)
    ) |> 
    filter(cumu_wl_max <= COD_WIN_THRESHOLD)
  
  sim_rounds <- df |> 
    pivot_longer(
      -i,
      names_to = 'r',
      values_to = 'w'
    ) |> 
    mutate(
      across(r, as.integer)
    ) |> 
    group_by(i) |> 
    mutate(
      cumu_w = cumsum(w),
      cumu_l = cumsum(w == 0)
    ) |> 
    ungroup() |> 
    mutate(
      cumu_wl_max = ifelse(cumu_w > cumu_l, cumu_w, cumu_l)
    ) |> 
    filter(cumu_wl_max <= COD_WIN_THRESHOLD)
  
  sim_rounds_to_drop <- anti_join(
    sim_rounds |> 
      filter(cumu_wl_max == COD_WIN_THRESHOLD),
    sim_rounds |> 
      filter(cumu_wl_max == COD_WIN_THRESHOLD) |> 
      group_by(i) |> 
      slice_min(r, n = 1) |> 
      ungroup(), 
    by = c('i', 'r', 'w', 'cumu_w', 'cumu_l', 'cumu_wl_max')
  )
  
  sim_rounds <- sim_rounds |> 
    anti_join(
      sim_rounds_to_drop, 
      by = c('i', 'r', 'w', 'cumu_w', 'cumu_l', 'cumu_wl_max')
    )
  
  ## always from offensive perspective
  pre_expected_records <- sim_rounds |> 
    inner_join(
      sim_rounds |> 
        filter(cumu_wl_max == COD_WIN_THRESHOLD) |> 
        mutate(
          zeros_win = cumu_l == cumu_wl_max
        ) |> 
        select(i, zeros_win),
      by = 'i'
    ) |> 
    mutate(
      across(w, ~ifelse(zeros_win, abs(1 - .x), .x))
    )
  
  pre_expected_records |> 
    group_by(i) |> 
    summarize(
      cumu_w = max(cumu_w),
      cumu_l = max(cumu_l),
      ws = paste0(w, collapse = '-')
    ) |> 
    ungroup() |> 
    mutate(
      wins = ifelse(cumu_w == COD_WIN_THRESHOLD, cumu_w, cumu_l),
      losses = ifelse(cumu_w == COD_WIN_THRESHOLD, cumu_l, cumu_w),
      n_rounds = wins + losses
    ) |> 
    count(n_rounds, ws, sort = TRUE) |> 
    mutate(prop = n / sum(n))
}

cod_expected_round_arrangements_naive <- simulate_cod_round_arrangements(0.5)
cod_expected_round_arrangements_mle <- simulate_cod_round_arrangements(0.575)

cod_actual_round_arrangements <- cod_rounds |> 
  filter(win_series) |> 
  mutate(across(win_round, as.integer)) |> 
  group_by(year, event, series) |> 
  summarize(
    wins = max(cumu_w),
    losses = max(cumu_l),
    ws = paste0(win_round, collapse = '-')
  ) |> 
  ungroup() |> 
  mutate(
    n_rounds = wins + losses
  ) |> 
  count(n_rounds,  ws, sort = TRUE) |> 
  mutate(prop = n / sum(n))

perform_run_tests <- function(cod_expected_round_arrangements) {
  suppressWarnings(
    cod_run_tests <- cod_expected_round_arrangements |> 
      select(n_rounds, ws) |> 
      separate(ws, into = as.character(1:11), remove = FALSE) |> 
      pivot_longer(
        -c(n_rounds, ws),
        names_to = 'round',
        values_to = 'w',
        values_drop_na = TRUE
      ) |> 
      mutate(
        across(c(round, w), as.integer)
      ) |> 
      select(-round) |> 
      group_by(n_rounds, ws) |> 
      summarize(
        w = list(w)
      ) |> 
      mutate(
        p_value = map_dbl(w, ~randtests::runs.test(.x, threshold = 0.5, pvalue = 'normal')$p.value),
        across(p_value, ~ifelse(n_rounds == 6, 0, .x)),
        is_significant = p_value <= 0.05
      ) |> 
      ungroup() |> 
      select(n_rounds, ws, p_value, is_significant)
  )
  
  expected_total <- sum(cod_expected_round_arrangements$n)
  actual_total <- sum(cod_actual_round_arrangements$n)
  cod_round_arrangements <- cod_expected_round_arrangements |> 
    rename(n_expected = n, prop_expected = prop) |>
    mutate(
      n_expected_adj = n_expected * (!!actual_total) / (!!expected_total)
    ) |> 
    left_join(
      cod_actual_round_arrangements |> 
        rename(n_actual = n, prop_actual = prop),
      by = c('n_rounds', 'ws')
    ) |> 
    inner_join(
      cod_run_tests,
      by = c('n_rounds', 'ws')
    ) |> 
    mutate(
      across(n_actual, replace_na, 0L),
      across(prop_actual, replace_na, 0)
    )
}

cod_round_arrangements_naive <- cod_expected_round_arrangements_naive |> perform_run_tests()
cod_round_arrangements_mle <- cod_expected_round_arrangements_mle |> perform_run_tests()

cod_round_arrangements_mle |> 
  filter(!is.na(p_value)) |> 
  filter(is_significant) |> 
  mutate(
    total_actual = sum(n_actual),
    total_expected_adj = sum(n_expected_adj)
  ) |>
  mutate(
    prop_p_value = vectorized_prop_test(
      n_actual, 
      total_actual, 
      round(n_expected_adj), 
      round(total_expected_adj)
    )$p.value,
    orig_prop_p_value = prop_p_value,
    across(prop_p_value, ~p.adjust(.x, method = 'BY') |> round(3))
  )
```

### Wald-Wolfowitz runs test

In Table \ref{tbl:ww-sequences}, the observed and expected count of sequences respectively, are shown for sequence for which we can reject the Wald-Wolfowitz null hypothesis[^20] at a confidence level of $\alpha = 0.05$, for $r \in [6, 7, 8, 9, 10]$[^21]. In addition to the 14 sequences shown, there are 12 additional sequences for $r = 11$.

[^20]: Interestingly, we identify the same non-random sequences when using either $\phi = 0.575$ or $\phi = 0.5$.

[^21]: Since the normal method is invalid for sequences with only successes or failures, we use the exact method to evaluate (and reject) the null hypothesis for the Wald Wolfowitz runs test for $r = 6$.

```{=tex}
\begin{table}

\caption{Sequences ($\zeta_j$) for which we can reject Wald-Wolfowitz null hypothesis for $r \in [6, 7, 8, 9, 10]$, where round wins and losses are denoted with $+$ and $-$ respectively. The observed count, $N^{\zeta}_j$, and proportion, $P^{\zeta}_j$, of all possible sequences, as well as the expected count, $\hat{N}^{\zeta}_j$, and proportion, $\hat{P}^{\zeta}_j$. $\hat{N}^{\zeta}_j$ is scaled to the observed number of series played, hence its non-integer value.}

\centering
\begin{tabular}{rlrrrr}
\toprule
$r$ & $\zeta_j$ & $N^{\zeta}_j$ & $P^{\zeta}_j$ & $\hat{N}^{\zeta}_j$ & $\hat{P}^{\zeta}_j$ \\ 
\midrule
6 & + + + + + + & 40 & 4.7\% & 32.9 & 3.86\% \\
8 & - - + + + + + + & 4 & 0.47\% & 7.67 & 0.90\% \\ 
9 & - - - + + + + + + & 4 & 0.47\% & 3.75 & 0.44\% \\ 
10 & + + + + - - - - + + & 1 & 0.12\% & 2.39 & 0.28\% \\ 
10 & + - - - - + + + + + & 0 & 0.00\% & 2.04 & 0.24\% \\ 
10 & + - + - + + - + - + & 3 & 0.35\% & 2.04 & 0.24\% \\ 
10 & + + + - - - - + + + & 2 & 0.23\% & 2.04 & 0.24\% \\ 
10 & - - - - + + + + + + & 2 & 0.23\% & 1.79 & 0.21\% \\ 
10 & + - + - + - + + - + & 3 & 0.35\% & 1.70 & 0.20\% \\ 
10 & + - + + - + - + - + & 0 & 0.00\% & 1.70 & 0.20\% \\ 
10 & + + - - - - + + + + & 1 & 0.12\% & 1.19 & 0.14\% \\ 
10 & + - + - + - + - + + & 3 & 0.35\% & 1.11 & 0.13\% \\ 
10 & + + - + - + - + - + & 0 & 0.00\% & 1.11 & 0.13\% \\ 
10 & + + + + + - - - - + & 2 & 0.23\% & 1.02 & 0.12\% \\ 

\bottomrule
\end{tabular}

\label{tbl:ww-sequences}
\end{table}
```
The expected proportion of occurrences of each sequence $\zeta_j$ is based on 10,000 simulations using the constant round probability $\phi_2 = 0.575$. A test for the difference between the observed and expected proportions of all sequences, $P^{\zeta}_j$ and $\hat{P}^{\zeta}_j$ respectively, indicates that the null hypothesis---that the two proportions are equal---cannot be rejected for any of the non-random sequences.

# Discussion

Anecdotally, most observers swear by the existence of momentum in CoD SnD series, to the extent that vernacular has been developed to describe such phenomenon. Viewers have come to embrace the "5-3" phenomenon, where teams win three consecutive rounds after facing a 5-3 deficit to win 6-5. There is even a term for the rare 0-5 comeback---a "full sail".

However, our results do not provide evidence for momentum on several fronts: (1) the evidence supporting the constant round probability hypothesis; (2) the failure to reject the binomial null hypothesis for post-streak win probabilities; and (3) the failure to reject the equal proportions null hypothesis for non-random sequences identified by the Wald-Wolfowitz runs test.

Perhaps it is not surprising that we did not find evidence in favor of the hot hand effect given the small "skill gap" in the CoD relative to other esports. (Most professional esports players who have played CoD, including CoD players themselves, would not hesitate to state this.) A small skill gap fosters randomness in outcomes, implying that a given team is less likely to go on streaks of consecutive round wins in SnD, compared to the counterfactual setting where the skill gap is "large" and better teams consistently defeat worse teams.

For the sake of brevity, we did not account for offensive or defensive role when considering win proportions in the rounds immediately following streaks. It would be interesting to look at whether the round win percentage is higher after streaks of round wins when the team starts the streak as the offensive team. Considering streaks of three, this would mean that the streaking team would be playing defense in the round immediately following the last streak win. Knowing that teams are more likely to win on defense---recall $\bar{\tau} = 47.8%$---we should expect that teams on such streaks are more likely to win the round than teams that go on streaks of three starting with a defensive round win. On the other hand, streaks of three starting with an offensive round win are less likely to occur than such streaks starting with a defensive win---$\bar{\tau}^2 (1 - \bar{\tau}) \approx$ 11.9% in the former case and $\bar{\tau} (1 - \bar{\tau})^2 \approx$ 13.0% in the latter case---so we would need to properly account for sample size differences.

Another idea would be to account for the quality of the teams, both in general and on specific maps. Even if we still cannot prove that the hot hand effect exists when doing so, a null finding would be insightful in and of itself.

Further, we have contacted the Twitter user "R11stats", who privately tracks in-round player engagements. Data regarding individual eliminations during each SnD round would facilitate research into player-specific momentum. While it seems there is no team-level hot hand effect in CoD SnD, perhaps there is with individual players.

# References {.unnumbered}

::: {#refs}
:::
